# üéì **–ü–æ–¥—Ä–æ–±–Ω–æ–µ —Ä—É–∫–æ–≤–æ–¥—Å—Ç–≤–æ –ø–æ –æ–±—É—á–µ–Ω–∏—é ForexBot AI**

## üìã **–°–æ–¥–µ—Ä–∂–∞–Ω–∏–µ**

1. [–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –∏–∑ MetaTrader5](#–ø–æ–¥–≥–æ—Ç–æ–≤–∫–∞-–¥–∞–Ω–Ω—ã—Ö-–∏–∑-metatrader5)
2. [–ù–∞—Å—Ç—Ä–æ–π–∫–∞ –æ–±—É—á–µ–Ω–∏—è](#–Ω–∞—Å—Ç—Ä–æ–π–∫–∞-–æ–±—É—á–µ–Ω–∏—è)
3. [–ü—Ä–æ—Ü–µ—Å—Å –æ–±—É—á–µ–Ω–∏—è](#–ø—Ä–æ—Ü–µ—Å—Å-–æ–±—É—á–µ–Ω–∏—è)
4. [–û—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –º–æ–¥–µ–ª–µ–π](#–æ—Ü–µ–Ω–∫–∞-–∫–∞—á–µ—Å—Ç–≤–∞-–º–æ–¥–µ–ª–µ–π)
5. [–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∏ –∑–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–µ–π](#—Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ-–∏-–∑–∞–≥—Ä—É–∑–∫–∞-–º–æ–¥–µ–ª–µ–π)
6. [–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤](#–æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è-–≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤)
7. [–ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –æ–±—É—á–µ–Ω–∏—è](#–º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥-–æ–±—É—á–µ–Ω–∏—è)
8. [–£—Å—Ç—Ä–∞–Ω–µ–Ω–∏–µ –ø—Ä–æ–±–ª–µ–º](#—É—Å—Ç—Ä–∞–Ω–µ–Ω–∏–µ-–ø—Ä–æ–±–ª–µ–º)

---

## üìä **1. –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –∏–∑ MetaTrader5**

### **üîß –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ MT5:**

```json
{
  "mt5": {
    "server": "YourBroker-Server",
    "login": 12345,
    "password": "your_password",
    "symbols": ["EURUSD", "GBPUSD", "USDJPY", "AUDUSD", "USDCHF", "NZDUSD", "USDCAD"],
    "timeframes": ["M1", "M5", "M15", "M30", "H1", "H4", "D1"]
  }
}
```

### **üìä –°–±–æ—Ä –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö:**

```bash
# –ó–∞–ø—É—Å–∫ —Å–±–æ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö
python data_collection/mt5_data_collector.py
```

**–†–µ–∫–æ–º–µ–Ω–¥—É–µ–º—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã —Å–±–æ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö:**

| –ü–∞—Ä–∞–º–µ—Ç—Ä | –ó–Ω–∞—á–µ–Ω–∏–µ | –û–ø–∏—Å–∞–Ω–∏–µ |
|----------|----------|----------|
| **–ü–µ—Ä–∏–æ–¥** | 1-3 –≥–æ–¥–∞ | –î–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è |
| **–¢–∞–π–º—Ñ—Ä–µ–π–º—ã** | M1, M5, M15, M30, H1, H4, D1 | –†–∞–∑–ª–∏—á–Ω—ã–µ –≤—Ä–µ–º–µ–Ω–Ω—ã–µ –º–∞—Å—à—Ç–∞–±—ã |
| **–°–∏–º–≤–æ–ª—ã** | Major pairs | –û—Å–Ω–æ–≤–Ω—ã–µ –≤–∞–ª—é—Ç–Ω—ã–µ –ø–∞—Ä—ã |
| **–ú–∏–Ω–∏–º—É–º –∑–∞–ø–∏—Å–µ–π** | 10,000 | –î–ª—è –∫–∞–∂–¥–æ–≥–æ —Å–∏–º–≤–æ–ª–∞/—Ç–∞–π–º—Ñ—Ä–µ–π–º–∞ |

### **üìà –¢—Ä–µ–±–æ–≤–∞–Ω–∏—è –∫ –∫–∞—á–µ—Å—Ç–≤—É –¥–∞–Ω–Ω—ã—Ö:**

```python
# –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –¥–∞–Ω–Ω—ã—Ö
def validate_data_quality(df):
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –ø—Ä–æ–ø—É—Å–∫–∏
    missing_values = df.isnull().sum()
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –¥—É–±–ª–∏–∫–∞—Ç—ã
    duplicates = df.index.duplicated().sum()
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –∞–Ω–æ–º–∞–ª–∏–∏
    negative_prices = (df[['open', 'high', 'low', 'close']] < 0).any(axis=1).sum()
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –æ–±—ä–µ–º–∞ –¥–∞–Ω–Ω—ã—Ö
    min_records = 10000
    
    return {
        'total_records': len(df),
        'missing_values': missing_values.to_dict(),
        'duplicates': duplicates,
        'negative_prices': negative_prices,
        'is_sufficient': len(df) >= min_records
    }
```

---

## ‚öôÔ∏è **2. –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –æ–±—É—á–µ–Ω–∏—è**

### **üìã –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –æ–±—É—á–µ–Ω–∏—è:**

```python
training_config = {
    'symbol': 'EURUSD',
    'timeframe': 'H1',
    'lookback_periods': [20, 50, 100, 200],
    'test_size': 0.2,
    'validation_size': 0.1,
    'sequence_length': 50,
    'batch_size': 32,
    'epochs': 100,
    'early_stopping_patience': 10,
    'min_samples': 10000,
    'feature_importance_threshold': 0.01,
    'class_balance_method': 'smote'  # 'smote', 'undersample', 'oversample'
}
```

### **üéØ –í—ã–±–æ—Ä —Ü–µ–ª–µ–≤–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π:**

```python
def create_target_variable(prices, threshold_buy=0.001, threshold_sell=-0.001):
    """
    –°–æ–∑–¥–∞–Ω–∏–µ —Ü–µ–ª–µ–≤–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏
    
    Args:
        prices: Series —Å —Ü–µ–Ω–∞–º–∏
        threshold_buy: –ü–æ—Ä–æ–≥ –¥–ª—è —Å–∏–≥–Ω–∞–ª–∞ –ø–æ–∫—É–ø–∫–∏ (0.1%)
        threshold_sell: –ü–æ—Ä–æ–≥ –¥–ª—è —Å–∏–≥–Ω–∞–ª–∞ –ø—Ä–æ–¥–∞–∂–∏ (-0.1%)
    
    Returns:
        target: –ú–∞—Å—Å–∏–≤ –∫–ª–∞—Å—Å–æ–≤ [0=HOLD, 1=BUY, 2=SELL]
    """
    # –†–∞—Å—á–µ—Ç –±—É–¥—É—â–∏—Ö –¥–æ—Ö–æ–¥–Ω–æ—Å—Ç–µ–π
    future_returns = prices.shift(-1) / prices - 1
    
    # –°–æ–∑–¥–∞–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤
    target = np.zeros(len(prices))
    target[future_returns > threshold_buy] = 1    # BUY
    target[future_returns < threshold_sell] = 2   # SELL
    # 0 = HOLD (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é)
    
    return target[:-1]  # –£–¥–∞–ª—è–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–π —ç–ª–µ–º–µ–Ω—Ç
```

### **üîß –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤:**

```python
# –°–æ–∑–¥–∞–Ω–∏–µ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–æ–≤
def create_features(df):
    # –ë–∞–∑–æ–≤—ã–µ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã
    df['sma_20'] = df['close'].rolling(window=20).mean()
    df['sma_50'] = df['close'].rolling(window=50).mean()
    df['ema_12'] = df['close'].ewm(span=12).mean()
    df['ema_26'] = df['close'].ewm(span=26).mean()
    
    # MACD
    df['macd'] = df['ema_12'] - df['ema_26']
    df['macd_signal'] = df['macd'].ewm(span=9).mean()
    df['macd_histogram'] = df['macd'] - df['macd_signal']
    
    # RSI
    delta = df['close'].diff()
    gain = (delta.where(delta > 0, 0)).rolling(window=14).mean()
    loss = (-delta.where(delta < 0, 0)).rolling(window=14).mean()
    rs = gain / loss
    df['rsi'] = 100 - (100 / (1 + rs))
    
    # Bollinger Bands
    df['bb_middle'] = df['close'].rolling(window=20).mean()
    bb_std = df['close'].rolling(window=20).std()
    df['bb_upper'] = df['bb_middle'] + (bb_std * 2)
    df['bb_lower'] = df['bb_middle'] - (bb_std * 2)
    
    # Stochastic
    low_min = df['low'].rolling(window=14).min()
    high_max = df['high'].rolling(window=14).max()
    df['stoch_k'] = 100 * ((df['close'] - low_min) / (high_max - low_min))
    df['stoch_d'] = df['stoch_k'].rolling(window=3).mean()
    
    # ATR
    high_low = df['high'] - df['low']
    high_close = np.abs(df['high'] - df['close'].shift())
    low_close = np.abs(df['low'] - df['close'].shift())
    true_range = np.maximum(high_low, np.maximum(high_close, low_close))
    df['atr'] = true_range.rolling(window=14).mean()
    
    return df
```

---

## üéì **3. –ü—Ä–æ—Ü–µ—Å—Å –æ–±—É—á–µ–Ω–∏—è**

### **üîÑ –ü–æ–ª–Ω—ã–π —Ü–∏–∫–ª –æ–±—É—á–µ–Ω–∏—è:**

```python
def train_model(symbol, timeframe):
    """
    –ü–æ–ª–Ω—ã–π —Ü–∏–∫–ª –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏
    
    Args:
        symbol: –í–∞–ª—é—Ç–Ω–∞—è –ø–∞—Ä–∞ (–Ω–∞–ø—Ä–∏–º–µ—Ä, 'EURUSD')
        timeframe: –¢–∞–π–º—Ñ—Ä–µ–π–º (–Ω–∞–ø—Ä–∏–º–µ—Ä, 'H1')
    
    Returns:
        bool: –£—Å–ø–µ—à–Ω–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è
    """
    
    # 1. –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
    df = load_market_data(symbol, timeframe)
    if df is None:
        return False
    
    # 2. –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö
    features, target, feature_columns = prepare_training_data(df)
    if features is None:
        return False
    
    # 3. –ë–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∞ –∫–ª–∞—Å—Å–æ–≤
    features_balanced, target_balanced = balance_classes(features, target)
    
    # 4. –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö
    X_train, X_val, X_test, y_test = split_data(features_balanced, target_balanced)
    
    # 5. –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π
    training_results = train_models(X_train, X_val, y_train, y_val)
    
    # 6. –û—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞
    evaluation_results = evaluate_models(X_test, y_test)
    
    # 7. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π
    save_models(symbol, timeframe)
    
    # 8. –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç—á–µ—Ç–∞
    generate_training_report(symbol, timeframe)
    
    return True
```

### **üìä –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö:**

```python
def split_data(features, target):
    """
    –í—Ä–µ–º–µ–Ω–Ω–æ–µ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã—Ö –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä—è–¥–æ–≤
    
    Args:
        features: –ü—Ä–∏–∑–Ω–∞–∫–∏
        target: –¶–µ–ª–µ–≤–∞—è –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è
    
    Returns:
        X_train, X_val, X_test, y_test
    """
    # –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ TimeSeriesSplit –¥–ª—è –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä—è–¥–æ–≤
    tscv = TimeSeriesSplit(n_splits=5)
    
    # –ü–æ–ª—É—á–∞–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–π —Å–ø–ª–∏—Ç
    for train_idx, test_idx in tscv.split(features):
        X_train, X_test = features[train_idx], features[test_idx]
        y_train, y_test = target[train_idx], target[test_idx]
    
    # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–µ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ –æ–±—É—á–∞—é—â–µ–π –≤—ã–±–æ—Ä–∫–∏
    split_point = int(len(X_train) * 0.9)  # 90% –¥–ª—è –æ–±—É—á–µ–Ω–∏—è, 10% –¥–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–∏
    X_train_final = X_train[:split_point]
    y_train_final = y_train[:split_point]
    X_val = X_train[split_point:]
    y_val = y_train[split_point:]
    
    return X_train_final, X_val, X_test, y_test
```

### **‚öñÔ∏è –ë–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∞ –∫–ª–∞—Å—Å–æ–≤:**

```python
def balance_classes(features, target, method='smote'):
    """
    –ë–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∞ –∫–ª–∞—Å—Å–æ–≤ –¥–ª—è —Ä–µ—à–µ–Ω–∏—è –ø—Ä–æ–±–ª–µ–º—ã –Ω–µ—Å–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω–æ—Å—Ç–∏
    
    Args:
        features: –ü—Ä–∏–∑–Ω–∞–∫–∏
        target: –¶–µ–ª–µ–≤–∞—è –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è
        method: –ú–µ—Ç–æ–¥ –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∏ ('smote', 'undersample', 'oversample')
    
    Returns:
        features_balanced, target_balanced
    """
    if method == 'smote':
        from imblearn.over_sampling import SMOTE
        smote = SMOTE(random_state=42)
        features_balanced, target_balanced = smote.fit_resample(features, target)
    
    elif method == 'undersample':
        from imblearn.under_sampling import RandomUnderSampler
        rus = RandomUnderSampler(random_state=42)
        features_balanced, target_balanced = rus.fit_resample(features, target)
    
    elif method == 'oversample':
        from imblearn.over_sampling import RandomOverSampler
        ros = RandomOverSampler(random_state=42)
        features_balanced, target_balanced = ros.fit_resample(features, target)
    
    else:
        features_balanced, target_balanced = features, target
    
    return features_balanced, target_balanced
```

---

## üìä **4. –û—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –º–æ–¥–µ–ª–µ–π**

### **üìà –ú–µ—Ç—Ä–∏–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞:**

```python
def evaluate_model_performance(y_true, y_pred, model_name):
    """
    –û—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –º–æ–¥–µ–ª–∏
    
    Args:
        y_true: –ò—Å—Ç–∏–Ω–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è
        y_pred: –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
        model_name: –ù–∞–∑–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏
    
    Returns:
        dict: –ú–µ—Ç—Ä–∏–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞
    """
    from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
    
    # –ë–∞–∑–æ–≤–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å
    accuracy = accuracy_score(y_true, y_pred)
    
    # –î–µ—Ç–∞–ª—å–Ω—ã–π –æ—Ç—á–µ—Ç
    class_report = classification_report(y_true, y_pred, output_dict=True)
    
    # –ú–∞—Ç—Ä–∏—Ü–∞ –æ—à–∏–±–æ–∫
    conf_matrix = confusion_matrix(y_true, y_pred)
    
    # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏
    precision = class_report['weighted avg']['precision']
    recall = class_report['weighted avg']['recall']
    f1_score = class_report['weighted avg']['f1-score']
    
    return {
        'model_name': model_name,
        'accuracy': accuracy,
        'precision': precision,
        'recall': recall,
        'f1_score': f1_score,
        'classification_report': class_report,
        'confusion_matrix': conf_matrix.tolist()
    }
```

### **üéØ –ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤:**

| –ú–µ—Ç—Ä–∏–∫–∞ | –û—Ç–ª–∏—á–Ω–æ–µ | –•–æ—Ä–æ—à–µ–µ | –£–¥–æ–≤–ª–µ—Ç–≤–æ—Ä–∏—Ç–µ–ª—å–Ω–æ–µ | –ü–ª–æ—Ö–æ–µ |
|---------|----------|---------|-------------------|--------|
| **Accuracy** | > 0.75 | 0.65-0.75 | 0.55-0.65 | < 0.55 |
| **Precision** | > 0.70 | 0.60-0.70 | 0.50-0.60 | < 0.50 |
| **Recall** | > 0.70 | 0.60-0.70 | 0.50-0.60 | < 0.50 |
| **F1-Score** | > 0.70 | 0.60-0.70 | 0.50-0.60 | < 0.50 |

### **üìä –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤:**

```python
def create_evaluation_plots(evaluation_results, save_path):
    """
    –°–æ–∑–¥–∞–Ω–∏–µ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–π –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –º–æ–¥–µ–ª–µ–π
    """
    import matplotlib.pyplot as plt
    import seaborn as sns
    
    # –ì—Ä–∞—Ñ–∏–∫ —Ç–æ—á–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–µ–π
    model_names = list(evaluation_results.keys())
    accuracies = [evaluation_results[name]['accuracy'] for name in model_names]
    
    plt.figure(figsize=(15, 10))
    
    # –ì—Ä–∞—Ñ–∏–∫ —Ç–æ—á–Ω–æ—Å—Ç–∏
    plt.subplot(2, 2, 1)
    plt.bar(model_names, accuracies)
    plt.title('–¢–æ—á–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–µ–π')
    plt.ylabel('–¢–æ—á–Ω–æ—Å—Ç—å')
    plt.xticks(rotation=45)
    
    # –ú–∞—Ç—Ä–∏—Ü—ã –æ—à–∏–±–æ–∫
    for i, model_name in enumerate(model_names[:3]):
        plt.subplot(2, 2, 2 + i)
        conf_matrix = np.array(evaluation_results[model_name]['confusion_matrix'])
        sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues')
        plt.title(f'–ú–∞—Ç—Ä–∏—Ü–∞ –æ—à–∏–±–æ–∫: {model_name}')
        plt.ylabel('–ò—Å—Ç–∏–Ω–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è')
        plt.xlabel('–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è')
    
    plt.tight_layout()
    plt.savefig(save_path, dpi=300, bbox_inches='tight')
    plt.close()
```

---

## üíæ **5. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∏ –∑–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–µ–π**

### **üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π:**

```python
def save_trained_models(training_results, symbol, timeframe):
    """
    –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –æ–±—É—á–µ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π
    
    Args:
        training_results: –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –æ–±—É—á–µ–Ω–∏—è
        symbol: –í–∞–ª—é—Ç–Ω–∞—è –ø–∞—Ä–∞
        timeframe: –¢–∞–π–º—Ñ—Ä–µ–π–º
    """
    import pickle
    import json
    from pathlib import Path
    
    # –°–æ–∑–¥–∞–Ω–∏–µ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
    model_dir = Path(f"data/models/{symbol}/{timeframe}")
    model_dir.mkdir(parents=True, exist_ok=True)
    
    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∫–∞–∂–¥–æ–π –º–æ–¥–µ–ª–∏
    for model_name, result in training_results.items():
        model = result['model']
        
        try:
            if hasattr(model, 'save'):
                # –ù–µ–π—Ä–æ–Ω–Ω—ã–µ —Å–µ—Ç–∏ (TensorFlow/Keras)
                model_path = model_dir / f"{model_name}.h5"
                model.save(str(model_path))
            else:
                # –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ –º–æ–¥–µ–ª–∏ (sklearn, XGBoost, LightGBM)
                model_path = model_dir / f"{model_name}.pkl"
                with open(model_path, 'wb') as f:
                    pickle.dump(model, f)
            
            print(f"‚úÖ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –º–æ–¥–µ–ª—å: {model_path}")
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è {model_name}: {e}")
    
    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö
    metadata = {
        'symbol': symbol,
        'timeframe': timeframe,
        'training_date': datetime.now().isoformat(),
        'model_performance': evaluation_results,
        'feature_columns': feature_columns
    }
    
    metadata_path = model_dir / 'training_metadata.json'
    with open(metadata_path, 'w') as f:
        json.dump(metadata, f, indent=2)
    
    print(f"‚úÖ –°–æ—Ö—Ä–∞–Ω–µ–Ω—ã –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ: {metadata_path}")
```

### **üìÇ –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–µ–π:**

```python
def load_trained_models(symbol, timeframe):
    """
    –ó–∞–≥—Ä—É–∑–∫–∞ –æ–±—É—á–µ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π
    
    Args:
        symbol: –í–∞–ª—é—Ç–Ω–∞—è –ø–∞—Ä–∞
        timeframe: –¢–∞–π–º—Ñ—Ä–µ–π–º
    
    Returns:
        dict: –ó–∞–≥—Ä—É–∂–µ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏
    """
    import pickle
    from pathlib import Path
    from tensorflow.keras.models import load_model
    
    model_dir = Path(f"data/models/{symbol}/{timeframe}")
    models = {}
    
    # –ó–∞–≥—Ä—É–∑–∫–∞ –∫–∞–∂–¥–æ–π –º–æ–¥–µ–ª–∏
    for model_file in model_dir.glob("*.h5"):
        model_name = model_file.stem
        try:
            model = load_model(str(model_file))
            models[model_name] = model
            print(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–∞ –º–æ–¥–µ–ª—å: {model_name}")
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ {model_name}: {e}")
    
    for model_file in model_dir.glob("*.pkl"):
        model_name = model_file.stem
        try:
            with open(model_file, 'rb') as f:
                model = pickle.load(f)
            models[model_name] = model
            print(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–∞ –º–æ–¥–µ–ª—å: {model_name}")
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ {model_name}: {e}")
    
    return models
```

---

## üîß **6. –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤**

### **üéØ Grid Search –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏:**

```python
def optimize_hyperparameters(X_train, y_train, model_type='xgboost'):
    """
    –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
    
    Args:
        X_train: –û–±—É—á–∞—é—â–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
        y_train: –û–±—É—á–∞—é—â–∞—è —Ü–µ–ª–µ–≤–∞—è –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è
        model_type: –¢–∏–ø –º–æ–¥–µ–ª–∏
    
    Returns:
        dict: –õ—É—á—à–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
    """
    from sklearn.model_selection import GridSearchCV
    from sklearn.metrics import make_scorer, accuracy_score
    
    if model_type == 'xgboost':
        from xgboost import XGBClassifier
        
        param_grid = {
            'n_estimators': [100, 200, 300],
            'max_depth': [3, 5, 7],
            'learning_rate': [0.01, 0.1, 0.2],
            'subsample': [0.8, 0.9, 1.0],
            'colsample_bytree': [0.8, 0.9, 1.0]
        }
        
        model = XGBClassifier(random_state=42)
    
    elif model_type == 'lightgbm':
        import lightgbm as lgb
        
        param_grid = {
            'n_estimators': [100, 200, 300],
            'max_depth': [3, 5, 7],
            'learning_rate': [0.01, 0.1, 0.2],
            'subsample': [0.8, 0.9, 1.0],
            'colsample_bytree': [0.8, 0.9, 1.0]
        }
        
        model = lgb.LGBMClassifier(random_state=42)
    
    # Grid Search
    grid_search = GridSearchCV(
        estimator=model,
        param_grid=param_grid,
        cv=TimeSeriesSplit(n_splits=3),
        scoring=make_scorer(accuracy_score),
        n_jobs=-1,
        verbose=1
    )
    
    grid_search.fit(X_train, y_train)
    
    return {
        'best_params': grid_search.best_params_,
        'best_score': grid_search.best_score_,
        'best_model': grid_search.best_estimator_
    }
```

### **üîç Bayesian Optimization:**

```python
def bayesian_optimization(X_train, y_train, model_type='xgboost'):
    """
    –ë–∞–π–µ—Å–æ–≤—Å–∫–∞—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
    """
    from skopt import gp_minimize
    from skopt.space import Real, Integer
    from sklearn.model_selection import cross_val_score
    
    def objective(params):
        if model_type == 'xgboost':
            from xgboost import XGBClassifier
            model = XGBClassifier(
                n_estimators=int(params[0]),
                max_depth=int(params[1]),
                learning_rate=params[2],
                subsample=params[3],
                colsample_bytree=params[4],
                random_state=42
            )
        
        # –ö—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏—è
        scores = cross_val_score(
            model, X_train, y_train, 
            cv=TimeSeriesSplit(n_splits=3), 
            scoring='accuracy'
        )
        
        return -scores.mean()  # –ú–∏–Ω–∏–º–∏–∑–∏—Ä—É–µ–º –æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—É—é —Ç–æ—á–Ω–æ—Å—Ç—å
    
    # –ü—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
    space = [
        Integer(50, 500, name='n_estimators'),
        Integer(3, 10, name='max_depth'),
        Real(0.01, 0.3, name='learning_rate'),
        Real(0.5, 1.0, name='subsample'),
        Real(0.5, 1.0, name='colsample_bytree')
    ]
    
    # –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è
    result = gp_minimize(
        objective, space, n_calls=50, 
        random_state=42, n_jobs=-1
    )
    
    return {
        'best_params': dict(zip(['n_estimators', 'max_depth', 'learning_rate', 'subsample', 'colsample_bytree'], result.x)),
        'best_score': -result.fun
    }
```

---

## üìä **7. –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –æ–±—É—á–µ–Ω–∏—è**

### **üìà –û—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏–µ –º–µ—Ç—Ä–∏–∫:**

```python
def monitor_training(history, model_name):
    """
    –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –ø—Ä–æ—Ü–µ—Å—Å–∞ –æ–±—É—á–µ–Ω–∏—è
    
    Args:
        history: –ò—Å—Ç–æ—Ä–∏—è –æ–±—É—á–µ–Ω–∏—è (–¥–ª—è –Ω–µ–π—Ä–æ–Ω–Ω—ã—Ö —Å–µ—Ç–µ–π)
        model_name: –ù–∞–∑–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏
    """
    import matplotlib.pyplot as plt
    
    plt.figure(figsize=(12, 4))
    
    # –ì—Ä–∞—Ñ–∏–∫ —Ç–æ—á–Ω–æ—Å—Ç–∏
    plt.subplot(1, 2, 1)
    plt.plot(history.history['accuracy'], label='Train Accuracy')
    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
    plt.title(f'Accuracy - {model_name}')
    plt.xlabel('Epoch')
    plt.ylabel('Accuracy')
    plt.legend()
    
    # –ì—Ä–∞—Ñ–∏–∫ –ø–æ—Ç–µ—Ä—å
    plt.subplot(1, 2, 2)
    plt.plot(history.history['loss'], label='Train Loss')
    plt.plot(history.history['val_loss'], label='Validation Loss')
    plt.title(f'Loss - {model_name}')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.legend()
    
    plt.tight_layout()
    plt.savefig(f'training_monitor_{model_name}.png', dpi=300, bbox_inches='tight')
    plt.close()
```

### **üîî –†–∞–Ω–Ω—è—è –æ—Å—Ç–∞–Ω–æ–≤–∫–∞:**

```python
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau

# Callbacks –¥–ª—è –Ω–µ–π—Ä–æ–Ω–Ω—ã—Ö —Å–µ—Ç–µ–π
callbacks = [
    EarlyStopping(
        monitor='val_loss',
        patience=10,
        restore_best_weights=True,
        verbose=1
    ),
    ReduceLROnPlateau(
        monitor='val_loss',
        factor=0.5,
        patience=5,
        min_lr=1e-7,
        verbose=1
    )
]

# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –≤ –æ–±—É—á–µ–Ω–∏–∏
history = model.fit(
    X_train, y_train,
    validation_data=(X_val, y_val),
    epochs=100,
    batch_size=32,
    callbacks=callbacks,
    verbose=1
)
```

---

## üîß **8. –£—Å—Ç—Ä–∞–Ω–µ–Ω–∏–µ –ø—Ä–æ–±–ª–µ–º**

### **‚ùå –ß–∞—Å—Ç—ã–µ –ø—Ä–æ–±–ª–µ–º—ã –∏ —Ä–µ—à–µ–Ω–∏—è:**

#### **1. –ü–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ (Overfitting):**
```python
# –†–µ—à–µ–Ω–∏—è:
# - –£–≤–µ–ª–∏—á–µ–Ω–∏–µ —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏–∏
# - –£–º–µ–Ω—å—à–µ–Ω–∏–µ —Å–ª–æ–∂–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–∏
# - –ë–æ–ª—å—à–µ –¥–∞–Ω–Ω—ã—Ö
# - –†–∞–Ω–Ω—è—è –æ—Å—Ç–∞–Ω–æ–≤–∫–∞

# –î–ª—è XGBoost:
model = XGBClassifier(
    reg_alpha=0.1,  # L1 —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è
    reg_lambda=1.0,  # L2 —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è
    max_depth=5,     # –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –≥–ª—É–±–∏–Ω—ã
    subsample=0.8,   # –°–ª—É—á–∞–π–Ω–∞—è –≤—ã–±–æ—Ä–∫–∞
    colsample_bytree=0.8  # –°–ª—É—á–∞–π–Ω–∞—è –≤—ã–±–æ—Ä–∫–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
)
```

#### **2. –ù–µ–¥–æ–æ–±—É—á–µ–Ω–∏–µ (Underfitting):**
```python
# –†–µ—à–µ–Ω–∏—è:
# - –£–≤–µ–ª–∏—á–µ–Ω–∏–µ —Å–ª–æ–∂–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–∏
# - –ë–æ–ª—å—à–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
# - –£–º–µ–Ω—å—à–µ–Ω–∏–µ —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏–∏
# - –ë–æ–ª—å—à–µ —ç–ø–æ—Ö –æ–±—É—á–µ–Ω–∏—è

# –î–ª—è –Ω–µ–π—Ä–æ–Ω–Ω—ã—Ö —Å–µ—Ç–µ–π:
model = Sequential([
    Dense(128, activation='relu', input_shape=(n_features,)),
    Dropout(0.3),
    Dense(64, activation='relu'),
    Dropout(0.3),
    Dense(32, activation='relu'),
    Dense(3, activation='softmax')
])
```

#### **3. –ù–µ—Å–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –∫–ª–∞—Å—Å—ã:**
```python
# –†–µ—à–µ–Ω–∏—è:
# - SMOTE –¥–ª—è –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∏
# - –í–∑–≤–µ—à–µ–Ω–Ω—ã–µ –∫–ª–∞—Å—Å—ã
# - –ò–∑–º–µ–Ω–µ–Ω–∏–µ –ø–æ—Ä–æ–≥–æ–≤ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏

# –í–∑–≤–µ—à–µ–Ω–Ω—ã–µ –∫–ª–∞—Å—Å—ã –¥–ª—è XGBoost:
class_weights = {
    0: 1.0,  # HOLD
    1: 2.0,  # BUY (–±–æ–ª—å—à–∏–π –≤–µ—Å)
    2: 2.0   # SELL (–±–æ–ª—å—à–∏–π –≤–µ—Å)
}

model = XGBClassifier(
    scale_pos_weight=2.0,  # –£–≤–µ–ª–∏—á–µ–Ω–∏–µ –≤–µ—Å–∞ –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã—Ö –∫–ª–∞—Å—Å–æ–≤
    class_weight=class_weights
)
```

#### **4. –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö:**
```python
# –†–µ—à–µ–Ω–∏—è:
# - –°–±–æ—Ä –±–æ–ª—å—à–µ–≥–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –¥–∞–Ω–Ω—ã—Ö
# - –ê—É–≥–º–µ–Ω—Ç–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö
# - –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π
# - Transfer learning

# –ê—É–≥–º–µ–Ω—Ç–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö:
def augment_data(df):
    # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ —à—É–º–∞ –∫ —Ü–µ–Ω–∞–º
    noise_factor = 0.001
    df_augmented = df.copy()
    df_augmented['close'] += np.random.normal(0, noise_factor, len(df))
    return df_augmented
```

### **üîç –î–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞ –ø—Ä–æ–±–ª–µ–º:**

```python
def diagnose_training_issues(X_train, y_train, X_val, y_val, model):
    """
    –î–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞ –ø—Ä–æ–±–ª–µ–º –æ–±—É—á–µ–Ω–∏—è
    """
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–∞–∑–º–µ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö
    print(f"–†–∞–∑–º–µ—Ä –æ–±—É—á–∞—é—â–µ–π –≤—ã–±–æ—Ä–∫–∏: {len(X_train)}")
    print(f"–†–∞–∑–º–µ—Ä –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω–æ–π –≤—ã–±–æ—Ä–∫–∏: {len(X_val)}")
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –∫–ª–∞—Å—Å–æ–≤
    print(f"–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤ (train): {np.bincount(y_train)}")
    print(f"–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤ (val): {np.bincount(y_val)}")
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
    print(f"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {X_train.shape[1]}")
    print(f"–ü—Ä–æ–ø—É—â–µ–Ω–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è: {np.isnan(X_train).sum()}")
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è
    train_score = model.score(X_train, y_train)
    val_score = model.score(X_val, y_val)
    
    print(f"–¢–æ—á–Ω–æ—Å—Ç—å –Ω–∞ –æ–±—É—á–∞—é—â–µ–π –≤—ã–±–æ—Ä–∫–µ: {train_score:.4f}")
    print(f"–¢–æ—á–Ω–æ—Å—Ç—å –Ω–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω–æ–π –≤—ã–±–æ—Ä–∫–µ: {val_score:.4f}")
    
    if train_score - val_score > 0.1:
        print("‚ö†Ô∏è –í–æ–∑–º–æ–∂–Ω–æ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ!")
    
    if val_score < 0.5:
        print("‚ö†Ô∏è –í–æ–∑–º–æ–∂–Ω–æ –Ω–µ–¥–æ–æ–±—É—á–µ–Ω–∏–µ!")
```

---

## üöÄ **–ë—ã—Å—Ç—Ä—ã–π —Å—Ç–∞—Ä—Ç –æ–±—É—á–µ–Ω–∏—è**

### **üìã –ü–æ—à–∞–≥–æ–≤—ã–π –ø–ª–∞–Ω:**

1. **–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö:**
   ```bash
   python data_collection/mt5_data_collector.py
   ```

2. **–ù–∞—Å—Ç—Ä–æ–π–∫–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏:**
   ```bash
   # –†–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ config.json
   nano config.json
   ```

3. **–ó–∞–ø—É—Å–∫ –æ–±—É—á–µ–Ω–∏—è:**
   ```bash
   python training/advanced_training_system.py
   ```

4. **–ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤:**
   ```bash
   # –ü—Ä–æ—Å–º–æ—Ç—Ä –æ—Ç—á–µ—Ç–æ–≤
   ls data/reports/
   
   # –ü—Ä–æ—Å–º–æ—Ç—Ä –º–æ–¥–µ–ª–µ–π
   ls data/models/
   ```

### **‚úÖ –ß–µ–∫-–ª–∏—Å—Ç —É—Å–ø–µ—à–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è:**

- [ ] –î–∞–Ω–Ω—ã–µ —Å–æ–±—Ä–∞–Ω—ã –∏ –ø—Ä–æ–≤–µ—Ä–µ–Ω—ã
- [ ] –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∞
- [ ] –ú–æ–¥–µ–ª–∏ –æ–±—É—á–µ–Ω—ã –±–µ–∑ –æ—à–∏–±–æ–∫
- [ ] –¢–æ—á–Ω–æ—Å—Ç—å > 60%
- [ ] –ù–µ—Ç –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è (—Ä–∞–∑–Ω–∏—Ü–∞ train/val < 10%)
- [ ] –ú–æ–¥–µ–ª–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã
- [ ] –û—Ç—á–µ—Ç—ã —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω—ã
- [ ] –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏ —Å–æ–∑–¥–∞–Ω—ã

**üéâ –ü–æ–∑–¥—Ä–∞–≤–ª—è–µ–º! –í–∞—à–∏ –º–æ–¥–µ–ª–∏ —É—Å–ø–µ—à–Ω–æ –æ–±—É—á–µ–Ω—ã –∏ –≥–æ—Ç–æ–≤—ã –∫ —Ç–æ—Ä–≥–æ–≤–ª–µ!**