"""
Progressive Model Training System
–ü–æ—ç—Ç–∞–ø–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π: XGBoost ‚Üí LSTM ‚Üí Ensemble
"""

import os
import sys
import asyncio
import time
import json
import pickle
from datetime import datetime
from typing import Dict, List, Tuple, Optional
import warnings

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, MinMaxScaler
from sklearn.metrics import mean_squared_error, mean_absolute_error, accuracy_score

# ML Models
import xgboost as xgb
import lightgbm as lgb

# Deep Learning
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Dropout, Bidirectional
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau

# Hyperparameter optimization
import optuna

# Visualization
import matplotlib.pyplot as plt
import seaborn as sns

# Custom imports
sys.path.append('..')
from trading_bot.mt5_connector.mt5_manager import MT5Manager
from loguru import logger

warnings.filterwarnings('ignore')


class ProgressiveForexTrainer:
    """–ü–æ—ç—Ç–∞–ø–Ω—ã–π —Ç—Ä–µ–Ω–µ—Ä –º–æ–¥–µ–ª–µ–π: XGBoost ‚Üí LSTM ‚Üí Ensemble"""
    
    def __init__(self, symbol: str = "EURUSD"):
        self.symbol = symbol
        self.models = {}
        self.scalers = {}
        self.results = {}
        self.training_history = []
        
        # –°–æ–∑–¥–∞–Ω–∏–µ –ø–∞–ø–æ–∫
        self.save_dir = f"progressive_models/{symbol}"
        os.makedirs(self.save_dir, exist_ok=True)
        
        # Setup GPU
        self._setup_gpu()
        
        logger.info(f"üöÄ –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω Progressive Trainer –¥–ª—è {symbol}")
    
    def _setup_gpu(self):
        """–ù–∞—Å—Ç—Ä–æ–π–∫–∞ GPU –¥–ª—è TensorFlow"""
        gpus = tf.config.experimental.list_physical_devices('GPU')
        if gpus:
            try:
                for gpu in gpus:
                    tf.config.experimental.set_memory_growth(gpu, True)
                logger.info(f"‚úÖ GPU –Ω–∞—Å—Ç—Ä–æ–µ–Ω: {len(gpus)} —É—Å—Ç—Ä–æ–π—Å—Ç–≤")
            except RuntimeError as e:
                logger.error(f"‚ùå –û—à–∏–±–∫–∞ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ GPU: {e}")
        else:
            logger.warning("‚ö†Ô∏è GPU –Ω–µ –Ω–∞–π–¥–µ–Ω, –∏—Å–ø–æ–ª—å–∑—É–µ–º CPU")
    
    def load_data(self, data_path: str = None) -> pd.DataFrame:
        """–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è"""
        
        if data_path and os.path.exists(data_path):
            logger.info(f"üìÇ –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ –∏–∑ {data_path}")
            df = pd.read_csv(data_path, index_col=0, parse_dates=True)
        else:
            # –°–æ–∑–¥–∞–Ω–∏–µ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏
            logger.info("üé≤ –°–æ–∑–¥–∞–µ–º —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏")
            df = self._create_demo_data()
        
        logger.info(f"üìä –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(df)} –∑–∞–ø–∏—Å–µ–π, {len(df.columns)} –ø—Ä–∏–∑–Ω–∞–∫–æ–≤")
        return df
    
    def _create_demo_data(self) -> pd.DataFrame:
        """–°–æ–∑–¥–∞–Ω–∏–µ –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–æ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö"""
        from datetime import timedelta
        
        # –í—Ä–µ–º–µ–Ω–Ω–æ–π —Ä—è–¥
        end_date = datetime.now()
        start_date = end_date - timedelta(days=365)
        dates = pd.date_range(start=start_date, end=end_date, freq='H')
        
        # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ü–µ–Ω
        np.random.seed(42)
        returns = np.random.normal(0, 0.001, len(dates))
        
        # –î–æ–±–∞–≤–ª—è–µ–º –∞–≤—Ç–æ–∫–æ—Ä—Ä–µ–ª—è—Ü–∏—é (—Ç—Ä–µ–Ω–¥–æ–≤–æ—Å—Ç—å)
        for i in range(1, len(returns)):
            returns[i] += 0.15 * returns[i-1]
        
        prices = 1.1000 + np.cumsum(returns)
        
        # OHLC –¥–∞–Ω–Ω—ã–µ
        df = pd.DataFrame({
            'open': prices,
            'high': prices * (1 + np.abs(np.random.normal(0, 0.0003, len(dates)))),
            'low': prices * (1 - np.abs(np.random.normal(0, 0.0003, len(dates)))),
            'close': prices,
            'volume': np.random.randint(1000, 50000, len(dates))
        }, index=dates)
        
        # –ö–æ—Ä—Ä–µ–∫—Ç–∏—Ä–æ–≤–∫–∞ high/low
        df['high'] = np.maximum(df[['open', 'close']].max(axis=1), df['high'])
        df['low'] = np.minimum(df[['open', 'close']].min(axis=1), df['low'])
        
        # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–æ–≤
        df = self._add_technical_indicators(df)
        
        # –¶–µ–ª–µ–≤–∞—è –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è (—Å–ª–µ–¥—É—é—â–∞—è —Ü–µ–Ω–∞)
        df['target'] = df['close'].shift(-1)
        df['target_direction'] = (df['target'] > df['close']).astype(int)
        
        return df.dropna()
    
    def _add_technical_indicators(self, df: pd.DataFrame) -> pd.DataFrame:
        """–î–æ–±–∞–≤–ª–µ–Ω–∏–µ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–æ–≤"""
        
        # –°–∫–æ–ª—å–∑—è—â–∏–µ —Å—Ä–µ–¥–Ω–∏–µ
        for period in [5, 10, 20, 50]:
            df[f'sma_{period}'] = df['close'].rolling(window=period).mean()
            df[f'ema_{period}'] = df['close'].ewm(span=period).mean()
        
        # RSI
        delta = df['close'].diff()
        gain = (delta.where(delta > 0, 0)).rolling(window=14).mean()
        loss = (-delta.where(delta < 0, 0)).rolling(window=14).mean()
        rs = gain / loss
        df['rsi'] = 100 - (100 / (1 + rs))
        
        # MACD
        df['macd'] = df['ema_12'] - df['ema_26'] if 'ema_12' in df.columns else df['close'].ewm(span=12).mean() - df['close'].ewm(span=26).mean()
        df['macd_signal'] = df['macd'].ewm(span=9).mean()
        
        # Bollinger Bands
        df['bb_middle'] = df['close'].rolling(window=20).mean()
        bb_std = df['close'].rolling(window=20).std()
        df['bb_upper'] = df['bb_middle'] + (bb_std * 2)
        df['bb_lower'] = df['bb_middle'] - (bb_std * 2)
        
        # –î–æ—Ö–æ–¥–Ω–æ—Å—Ç–∏ –∏ –≤–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç—å
        df['returns'] = df['close'].pct_change()
        df['volatility'] = df['returns'].rolling(window=20).std()
        
        # –õ–∞–≥–æ–≤—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
        for lag in [1, 2, 3, 5, 10]:
            df[f'close_lag_{lag}'] = df['close'].shift(lag)
            df[f'returns_lag_{lag}'] = df['returns'].shift(lag)
        
        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
        df['high_low_ratio'] = df['high'] / df['low']
        df['price_position'] = (df['close'] - df['bb_lower']) / (df['bb_upper'] - df['bb_lower'])
        
        return df
    
    def prepare_data(self, df: pd.DataFrame) -> Tuple[np.ndarray, np.ndarray, List[str]]:
        """–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è"""
        
        # –í—ã–±–æ—Ä –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ (–∏—Å–∫–ª—é—á–∞–µ–º —Ü–µ–ª–µ–≤—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ)
        feature_columns = [col for col in df.columns 
                          if not col.startswith('target') and col not in ['open', 'high', 'low', 'close']]
        
        X = df[feature_columns].values
        y = df['target'].values
        
        # –£–¥–∞–ª–µ–Ω–∏–µ NaN
        mask = ~(np.isnan(X).any(axis=1) | np.isnan(y))
        X = X[mask]
        y = y[mask]
        
        logger.info(f"üìù –ü–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–æ {len(X)} –æ–±—Ä–∞–∑—Ü–æ–≤, {len(feature_columns)} –ø—Ä–∏–∑–Ω–∞–∫–æ–≤")
        
        return X, y, feature_columns
    
    # ================== –≠–¢–ê–ü 1: XGBoost ==================
    
    def train_xgboost_phase(self, X: np.ndarray, y: np.ndarray, 
                           optimize: bool = True, trials: int = 50) -> Dict:
        """–≠–¢–ê–ü 1: –û–±—É—á–µ–Ω–∏–µ XGBoost - –±—ã—Å—Ç—Ä–æ –∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ"""
        
        logger.info("üéØ –≠–¢–ê–ü 1: –û–±—É—á–µ–Ω–∏–µ XGBoost (–±—ã—Å—Ç—Ä–æ –∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ)")
        start_time = time.time()
        
        # –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.2, shuffle=False  # –í—Ä–µ–º–µ–Ω–Ω—ã–µ —Ä—è–¥—ã –Ω–µ –ø–µ—Ä–µ–º–µ—à–∏–≤–∞–µ–º
        )
        X_train, X_val, y_train, y_val = train_test_split(
            X_train, y_train, test_size=0.2, shuffle=False
        )
        
        # –ú–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ
        scaler = StandardScaler()
        X_train_scaled = scaler.fit_transform(X_train)
        X_val_scaled = scaler.transform(X_val)
        X_test_scaled = scaler.transform(X_test)
        self.scalers['xgboost'] = scaler
        
        if optimize:
            logger.info(f"üîß –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ ({trials} trials)...")
            model = self._optimize_xgboost(X_train_scaled, y_train, X_val_scaled, y_val, trials)
        else:
            logger.info("‚ö° –ë—ã—Å—Ç—Ä–æ–µ –æ–±—É—á–µ–Ω–∏–µ —Å —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏...")
            model = xgb.XGBRegressor(
                n_estimators=200,
                max_depth=6,
                learning_rate=0.1,
                random_state=42
            )
            model.fit(X_train_scaled, y_train)
        
        # –û—Ü–µ–Ω–∫–∞
        train_pred = model.predict(X_train_scaled)
        val_pred = model.predict(X_val_scaled)
        test_pred = model.predict(X_test_scaled)
        
        results = {
            'model': model,
            'train_mse': mean_squared_error(y_train, train_pred),
            'val_mse': mean_squared_error(y_val, val_pred),
            'test_mse': mean_squared_error(y_test, test_pred),
            'test_mae': mean_absolute_error(y_test, test_pred),
            'directional_accuracy': self._calculate_directional_accuracy(y_test, test_pred),
            'predictions': test_pred,
            'actual': y_test,
            'training_time': time.time() - start_time
        }
        
        self.models['xgboost'] = model
        self.results['xgboost'] = results
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ
        self._save_model('xgboost', model, results)
        
        logger.info(f"‚úÖ XGBoost –∑–∞–≤–µ—Ä—à–µ–Ω –∑–∞ {results['training_time']:.1f}—Å")
        logger.info(f"üìä Test MSE: {results['test_mse']:.6f}")
        logger.info(f"üéØ Directional Accuracy: {results['directional_accuracy']:.1f}%")
        
        return results
    
    def _optimize_xgboost(self, X_train: np.ndarray, y_train: np.ndarray,
                         X_val: np.ndarray, y_val: np.ndarray, trials: int) -> xgb.XGBRegressor:
        """–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è XGBoost —Å Optuna"""
        
        def objective(trial):
            params = {
                'n_estimators': trial.suggest_int('n_estimators', 100, 500),
                'max_depth': trial.suggest_int('max_depth', 3, 10),
                'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3),
                'subsample': trial.suggest_float('subsample', 0.7, 1.0),
                'colsample_bytree': trial.suggest_float('colsample_bytree', 0.7, 1.0),
                'random_state': 42
            }
            
            model = xgb.XGBRegressor(**params)
            model.fit(X_train, y_train, 
                     eval_set=[(X_val, y_val)], 
                     early_stopping_rounds=20, 
                     verbose=False)
            
            y_pred = model.predict(X_val)
            return mean_squared_error(y_val, y_pred)
        
        study = optuna.create_study(direction='minimize')
        study.optimize(objective, n_trials=trials)
        
        best_model = xgb.XGBRegressor(**study.best_params, random_state=42)
        best_model.fit(X_train, y_train)
        
        logger.info(f"üèÜ –õ—É—á—à–∏–π MSE: {study.best_value:.6f}")
        
        return best_model
    
    # ================== –≠–¢–ê–ü 2: LSTM ==================
    
    def train_lstm_phase(self, X: np.ndarray, y: np.ndarray, 
                        sequence_length: int = 60, optimize: bool = True, trials: int = 30) -> Dict:
        """–≠–¢–ê–ü 2: –û–±—É—á–µ–Ω–∏–µ LSTM - –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Ç–æ—á–Ω–æ—Å—Ç–∏"""
        
        logger.info("üß† –≠–¢–ê–ü 2: –û–±—É—á–µ–Ω–∏–µ LSTM (–¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Ç–æ—á–Ω–æ—Å—Ç–∏)")
        start_time = time.time()
        
        # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è LSTM
        X_lstm, y_lstm = self._prepare_lstm_data(X, y, sequence_length)
        
        # –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö
        split_idx = int(len(X_lstm) * 0.8)
        val_split_idx = int(split_idx * 0.8)
        
        X_train = X_lstm[:val_split_idx]
        X_val = X_lstm[val_split_idx:split_idx]
        X_test = X_lstm[split_idx:]
        
        y_train = y_lstm[:val_split_idx]
        y_val = y_lstm[val_split_idx:split_idx]
        y_test = y_lstm[split_idx:]
        
        if optimize:
            logger.info(f"üîß –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è LSTM ({trials} trials)...")
            model = self._optimize_lstm(X_train, y_train, X_val, y_val, trials)
        else:
            logger.info("‚ö° –ë—ã—Å—Ç—Ä–æ–µ –æ–±—É—á–µ–Ω–∏–µ LSTM...")
            model = self._create_lstm_model(X_train.shape[1:])
            
            callbacks = [
                EarlyStopping(patience=10, restore_best_weights=True),
                ReduceLROnPlateau(factor=0.5, patience=5)
            ]
            
            model.fit(X_train, y_train,
                     validation_data=(X_val, y_val),
                     epochs=50, batch_size=32,
                     callbacks=callbacks, verbose=1)
        
        # –û—Ü–µ–Ω–∫–∞
        train_pred = model.predict(X_train).flatten()
        val_pred = model.predict(X_val).flatten()
        test_pred = model.predict(X_test).flatten()
        
        results = {
            'model': model,
            'train_mse': mean_squared_error(y_train, train_pred),
            'val_mse': mean_squared_error(y_val, val_pred),
            'test_mse': mean_squared_error(y_test, test_pred),
            'test_mae': mean_absolute_error(y_test, test_pred),
            'directional_accuracy': self._calculate_directional_accuracy(y_test, test_pred),
            'predictions': test_pred,
            'actual': y_test,
            'training_time': time.time() - start_time
        }
        
        self.models['lstm'] = model
        self.results['lstm'] = results
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ
        self._save_model('lstm', model, results)
        
        logger.info(f"‚úÖ LSTM –∑–∞–≤–µ—Ä—à–µ–Ω –∑–∞ {results['training_time']:.1f}—Å")
        logger.info(f"üìä Test MSE: {results['test_mse']:.6f}")
        logger.info(f"üéØ Directional Accuracy: {results['directional_accuracy']:.1f}%")
        
        return results
    
    def _prepare_lstm_data(self, X: np.ndarray, y: np.ndarray, 
                          sequence_length: int) -> Tuple[np.ndarray, np.ndarray]:
        """–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è LSTM"""
        
        # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è
        scaler = MinMaxScaler()
        X_scaled = scaler.fit_transform(X)
        self.scalers['lstm'] = scaler
        
        # –°–æ–∑–¥–∞–Ω–∏–µ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π
        X_sequences, y_sequences = [], []
        
        for i in range(sequence_length, len(X_scaled)):
            X_sequences.append(X_scaled[i-sequence_length:i])
            y_sequences.append(y[i])
        
        return np.array(X_sequences), np.array(y_sequences)
    
    def _create_lstm_model(self, input_shape: Tuple) -> tf.keras.Model:
        """–°–æ–∑–¥–∞–Ω–∏–µ LSTM –º–æ–¥–µ–ª–∏"""
        
        model = Sequential([
            Bidirectional(LSTM(64, return_sequences=True), input_shape=input_shape),
            Dropout(0.2),
            Bidirectional(LSTM(32, return_sequences=True)),
            Dropout(0.2),
            Bidirectional(LSTM(16)),
            Dropout(0.2),
            Dense(25, activation='relu'),
            Dense(1)
        ])
        
        model.compile(
            optimizer=Adam(learning_rate=0.001),
            loss='mse',
            metrics=['mae']
        )
        
        return model
    
    def _optimize_lstm(self, X_train: np.ndarray, y_train: np.ndarray,
                      X_val: np.ndarray, y_val: np.ndarray, trials: int) -> tf.keras.Model:
        """–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è LSTM —Å Optuna"""
        
        def objective(trial):
            # –ì–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã
            units_1 = trial.suggest_int('units_1', 32, 128)
            units_2 = trial.suggest_int('units_2', 16, 64)
            dropout = trial.suggest_float('dropout', 0.1, 0.5)
            learning_rate = trial.suggest_float('learning_rate', 0.0001, 0.01)
            
            # –ú–æ–¥–µ–ª—å
            model = Sequential([
                Bidirectional(LSTM(units_1, return_sequences=True), input_shape=X_train.shape[1:]),
                Dropout(dropout),
                Bidirectional(LSTM(units_2)),
                Dropout(dropout),
                Dense(25, activation='relu'),
                Dense(1)
            ])
            
            model.compile(
                optimizer=Adam(learning_rate=learning_rate),
                loss='mse'
            )
            
            # –û–±—É—á–µ–Ω–∏–µ
            history = model.fit(
                X_train, y_train,
                validation_data=(X_val, y_val),
                epochs=30,  # –ú–µ–Ω—å—à–µ —ç–ø–æ—Ö –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
                batch_size=32,
                verbose=0,
                callbacks=[EarlyStopping(patience=5, restore_best_weights=True)]
            )
            
            return min(history.history['val_loss'])
        
        study = optuna.create_study(direction='minimize')
        study.optimize(objective, n_trials=trials)
        
        # –§–∏–Ω–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å —Å –ª—É—á—à–∏–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏
        best_model = self._create_lstm_model(X_train.shape[1:])
        
        callbacks = [
            EarlyStopping(patience=15, restore_best_weights=True),
            ReduceLROnPlateau(factor=0.5, patience=5)
        ]
        
        best_model.fit(X_train, y_train,
                      validation_data=(X_val, y_val),
                      epochs=100, batch_size=32,
                      callbacks=callbacks, verbose=1)
        
        logger.info(f"üèÜ –õ—É—á—à–∏–π val_loss: {study.best_value:.6f}")
        
        return best_model
    
    # ================== –≠–¢–ê–ü 3: ENSEMBLE ==================
    
    def train_ensemble_phase(self, X: np.ndarray, y: np.ndarray) -> Dict:
        """–≠–¢–ê–ü 3: Ensemble - –º–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å"""
        
        logger.info("üèÜ –≠–¢–ê–ü 3: Ensemble (–º–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å)")
        start_time = time.time()
        
        if 'xgboost' not in self.models or 'lstm' not in self.models:
            logger.error("‚ùå –ù—É–∂–Ω–æ —Å–Ω–∞—á–∞–ª–∞ –æ–±—É—á–∏—Ç—å XGBoost –∏ LSTM")
            return {}
        
        # –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö (—Ç–µ –∂–µ —Ä–∞–∑–±–∏–µ–Ω–∏—è)
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.2, shuffle=False
        )
        
        # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è XGBoost
        X_train_xgb = self.scalers['xgboost'].transform(X_train)
        X_test_xgb = self.scalers['xgboost'].transform(X_test)
        
        # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è LSTM
        X_lstm_full, y_lstm_full = self._prepare_lstm_data(X, y, 60)
        split_idx = int(len(X_lstm_full) * 0.8)
        X_test_lstm = X_lstm_full[split_idx:]
        y_test_lstm = y_lstm_full[split_idx:]
        
        # –ü–æ–ª—É—á–µ–Ω–∏–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π –æ—Ç –∫–∞–∂–¥–æ–π –º–æ–¥–µ–ª–∏
        xgb_pred = self.models['xgboost'].predict(X_test_xgb)
        lstm_pred = self.models['lstm'].predict(X_test_lstm).flatten()
        
        # –í—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏–µ —Ä–∞–∑–º–µ—Ä–æ–≤ (LSTM –º–æ–∂–µ—Ç –∏–º–µ—Ç—å –º–µ–Ω—å—à–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π)
        min_length = min(len(xgb_pred), len(lstm_pred), len(y_test), len(y_test_lstm))
        xgb_pred = xgb_pred[:min_length]
        lstm_pred = lstm_pred[:min_length]
        y_test_aligned = y_test[:min_length]
        
        # –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –≤–µ—Å–æ–≤ –∞–Ω—Å–∞–º–±–ª—è
        logger.info("üîß –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –≤–µ—Å–æ–≤ –∞–Ω—Å–∞–º–±–ª—è...")
        best_weights = self._optimize_ensemble_weights(
            xgb_pred, lstm_pred, y_test_aligned
        )
        
        # –§–∏–Ω–∞–ª—å–Ω—ã–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –∞–Ω—Å–∞–º–±–ª—è
        ensemble_pred = (best_weights[0] * xgb_pred + 
                        best_weights[1] * lstm_pred)
        
        results = {
            'weights': best_weights,
            'test_mse': mean_squared_error(y_test_aligned, ensemble_pred),
            'test_mae': mean_absolute_error(y_test_aligned, ensemble_pred),
            'directional_accuracy': self._calculate_directional_accuracy(y_test_aligned, ensemble_pred),
            'predictions': ensemble_pred,
            'actual': y_test_aligned,
            'xgb_predictions': xgb_pred,
            'lstm_predictions': lstm_pred,
            'training_time': time.time() - start_time
        }
        
        self.results['ensemble'] = results
        
        # –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Å –∏–Ω–¥–∏–≤–∏–¥—É–∞–ª—å–Ω—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏
        xgb_mse = mean_squared_error(y_test_aligned, xgb_pred)
        lstm_mse = mean_squared_error(y_test_aligned, lstm_pred)
        
        logger.info(f"‚úÖ Ensemble –∑–∞–≤–µ—Ä—à–µ–Ω –∑–∞ {results['training_time']:.1f}—Å")
        logger.info(f"üìä –°—Ä–∞–≤–Ω–µ–Ω–∏–µ MSE:")
        logger.info(f"   XGBoost: {xgb_mse:.6f}")
        logger.info(f"   LSTM:    {lstm_mse:.6f}")
        logger.info(f"   Ensemble: {results['test_mse']:.6f}")
        logger.info(f"üéØ Ensemble Directional Accuracy: {results['directional_accuracy']:.1f}%")
        logger.info(f"‚öñÔ∏è –û–ø—Ç–∏–º–∞–ª—å–Ω—ã–µ –≤–µ—Å–∞: XGBoost={best_weights[0]:.2f}, LSTM={best_weights[1]:.2f}")
        
        return results
    
    def _optimize_ensemble_weights(self, pred1: np.ndarray, pred2: np.ndarray, 
                                  y_true: np.ndarray) -> Tuple[float, float]:
        """–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –≤–µ—Å–æ–≤ –∞–Ω—Å–∞–º–±–ª—è"""
        
        def objective(trial):
            w1 = trial.suggest_float('weight_xgb', 0.0, 1.0)
            w2 = 1.0 - w1
            
            ensemble_pred = w1 * pred1 + w2 * pred2
            return mean_squared_error(y_true, ensemble_pred)
        
        study = optuna.create_study(direction='minimize')
        study.optimize(objective, n_trials=100)
        
        best_w1 = study.best_params['weight_xgb']
        best_w2 = 1.0 - best_w1
        
        return (best_w1, best_w2)
    
    # ================== UTILITIES ==================
    
    def _calculate_directional_accuracy(self, y_true: np.ndarray, y_pred: np.ndarray) -> float:
        """–†–∞—Å—á–µ—Ç —Ç–æ—á–Ω–æ—Å—Ç–∏ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è"""
        if len(y_true) <= 1:
            return 0.0
        
        true_direction = np.diff(y_true) > 0
        pred_direction = np.diff(y_pred) > 0
        
        return np.mean(true_direction == pred_direction) * 100
    
    def _save_model(self, model_name: str, model, results: Dict):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤"""
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
        if model_name == 'lstm':
            model.save(f"{self.save_dir}/{model_name}_model.h5")
        else:
            with open(f"{self.save_dir}/{model_name}_model.pkl", 'wb') as f:
                pickle.dump(model, f)
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        results_to_save = {k: v for k, v in results.items() 
                          if k not in ['model', 'predictions', 'actual']}
        
        with open(f"{self.save_dir}/{model_name}_results.json", 'w') as f:
            json.dump(results_to_save, f, indent=2)
        
        logger.info(f"üíæ {model_name} —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤ {self.save_dir}")
    
    def create_comparison_plots(self):
        """–°–æ–∑–¥–∞–Ω–∏–µ –≥—Ä–∞—Ñ–∏–∫–æ–≤ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π"""
        
        if not self.results:
            logger.warning("‚ö†Ô∏è –ù–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –¥–ª—è –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏—è –≥—Ä–∞—Ñ–∏–∫–æ–≤")
            return
        
        plt.figure(figsize=(20, 12))
        
        # 1. –°—Ä–∞–≤–Ω–µ–Ω–∏–µ MSE
        plt.subplot(2, 3, 1)
        models = list(self.results.keys())
        mse_values = [self.results[model]['test_mse'] for model in models]
        
        bars = plt.bar(models, mse_values, color=['#ff7f0e', '#2ca02c', '#d62728'])
        plt.title('–°—Ä–∞–≤–Ω–µ–Ω–∏–µ MSE –º–æ–¥–µ–ª–µ–π', fontsize=14, fontweight='bold')
        plt.ylabel('MSE')
        
        # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –∑–Ω–∞—á–µ–Ω–∏–π –Ω–∞ —Å—Ç–æ–ª–±—Ü—ã
        for bar, value in zip(bars, mse_values):
            plt.text(bar.get_x() + bar.get_width()/2, bar.get_height(),
                    f'{value:.6f}', ha='center', va='bottom')
        
        # 2. –°—Ä–∞–≤–Ω–µ–Ω–∏–µ Directional Accuracy
        plt.subplot(2, 3, 2)
        acc_values = [self.results[model]['directional_accuracy'] for model in models]
        
        bars = plt.bar(models, acc_values, color=['#ff7f0e', '#2ca02c', '#d62728'])
        plt.title('–¢–æ—á–Ω–æ—Å—Ç—å –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è (%)', fontsize=14, fontweight='bold')
        plt.ylabel('Accuracy (%)')
        plt.ylim(40, 70)
        
        for bar, value in zip(bars, acc_values):
            plt.text(bar.get_x() + bar.get_width()/2, bar.get_height(),
                    f'{value:.1f}%', ha='center', va='bottom')
        
        # 3. –í—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è
        plt.subplot(2, 3, 3)
        time_values = [self.results[model]['training_time'] / 60 for model in models]  # –í –º–∏–Ω—É—Ç–∞—Ö
        
        bars = plt.bar(models, time_values, color=['#ff7f0e', '#2ca02c', '#d62728'])
        plt.title('–í—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è (–º–∏–Ω—É—Ç—ã)', fontsize=14, fontweight='bold')
        plt.ylabel('–ú–∏–Ω—É—Ç—ã')
        
        for bar, value in zip(bars, time_values):
            plt.text(bar.get_x() + bar.get_width()/2, bar.get_height(),
                    f'{value:.1f}–º', ha='center', va='bottom')
        
        # 4. –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è vs –†–µ–∞–ª—å–Ω–æ—Å—Ç—å (–µ—Å–ª–∏ –µ—Å—Ç—å ensemble)
        if 'ensemble' in self.results:
            plt.subplot(2, 3, 4)
            ensemble_results = self.results['ensemble']
            
            plt.plot(ensemble_results['actual'][:100], label='–†–µ–∞–ª—å–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è', alpha=0.7)
            plt.plot(ensemble_results['predictions'][:100], label='Ensemble', alpha=0.7)
            plt.plot(ensemble_results['xgb_predictions'][:100], label='XGBoost', alpha=0.5)
            plt.plot(ensemble_results['lstm_predictions'][:100], label='LSTM', alpha=0.5)
            
            plt.title('–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è vs –†–µ–∞–ª—å–Ω–æ—Å—Ç—å (–ø–µ—Ä–≤—ã–µ 100 —Ç–æ—á–µ–∫)', fontsize=14, fontweight='bold')
            plt.legend()
            plt.grid(True, alpha=0.3)
        
        # 5. –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –æ—à–∏–±–æ–∫
        plt.subplot(2, 3, 5)
        for model in models:
            if 'predictions' in self.results[model] and 'actual' in self.results[model]:
                errors = self.results[model]['actual'] - self.results[model]['predictions']
                plt.hist(errors, bins=30, alpha=0.5, label=model, density=True)
        
        plt.title('–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –æ—à–∏–±–æ–∫', fontsize=14, fontweight='bold')
        plt.xlabel('–û—à–∏–±–∫–∞ –ø—Ä–æ–≥–Ω–æ–∑–∞')
        plt.ylabel('–ü–ª–æ—Ç–Ω–æ—Å—Ç—å')
        plt.legend()
        plt.grid(True, alpha=0.3)
        
        # 6. –í–µ—Å–∞ –∞–Ω—Å–∞–º–±–ª—è (–µ—Å–ª–∏ –µ—Å—Ç—å)
        if 'ensemble' in self.results:
            plt.subplot(2, 3, 6)
            weights = self.results['ensemble']['weights']
            models_ensemble = ['XGBoost', 'LSTM']
            
            wedges, texts, autotexts = plt.pie(weights, labels=models_ensemble, autopct='%1.1f%%',
                                              colors=['#ff7f0e', '#2ca02c'])
            plt.title('–í–µ—Å–∞ –≤ –∞–Ω—Å–∞–º–±–ª–µ', fontsize=14, fontweight='bold')
        
        plt.tight_layout()
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ
        plot_path = f"{self.save_dir}/comparison_results.png"
        plt.savefig(plot_path, dpi=300, bbox_inches='tight')
        logger.info(f"üìä –ì—Ä–∞—Ñ–∏–∫–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ {plot_path}")
        
        plt.show()
    
    def print_final_summary(self):
        """–í—ã–≤–æ–¥ —Ñ–∏–Ω–∞–ª—å–Ω–æ–≥–æ —Ä–µ–∑—é–º–µ"""
        
        logger.info("\n" + "="*80)
        logger.info("üèÅ –§–ò–ù–ê–õ–¨–ù–û–ï –†–ï–ó–Æ–ú–ï –û–ë–£–ß–ï–ù–ò–Ø")
        logger.info("="*80)
        
        if not self.results:
            logger.warning("‚ö†Ô∏è –ù–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –¥–ª—è –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è")
            return
        
        total_time = sum(result['training_time'] for result in self.results.values())
        
        logger.info(f"üìä –°–∏–º–≤–æ–ª: {self.symbol}")
        logger.info(f"‚è±Ô∏è –û–±—â–µ–µ –≤—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è: {total_time:.1f} —Å–µ–∫—É–Ω–¥ ({total_time/60:.1f} –º–∏–Ω—É—Ç)")
        logger.info(f"üíæ –ú–æ–¥–µ–ª–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤: {self.save_dir}")
        
        logger.info("\nüìà –†–ï–ó–£–õ–¨–¢–ê–¢–´ –ü–û –≠–¢–ê–ü–ê–ú:")
        
        for i, (model_name, results) in enumerate(self.results.items(), 1):
            stage_names = {
                'xgboost': '–≠–¢–ê–ü 1: XGBoost (–ë—ã—Å—Ç—Ä–æ –∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ)',
                'lstm': '–≠–¢–ê–ü 2: LSTM (–£–ª—É—á—à–µ–Ω–∏–µ —Ç–æ—á–Ω–æ—Å—Ç–∏)', 
                'ensemble': '–≠–¢–ê–ü 3: Ensemble (–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å)'
            }
            
            logger.info(f"\n{stage_names.get(model_name, model_name)}:")
            logger.info(f"   ‚è±Ô∏è –í—Ä–µ–º—è: {results['training_time']:.1f}—Å")
            logger.info(f"   üìä MSE: {results['test_mse']:.6f}")
            logger.info(f"   üéØ Directional Accuracy: {results['directional_accuracy']:.1f}%")
            
            if model_name == 'ensemble':
                weights = results['weights']
                logger.info(f"   ‚öñÔ∏è –í–µ—Å–∞: XGBoost={weights[0]:.2f}, LSTM={weights[1]:.2f}")
        
        # –õ—É—á—à–∞—è –º–æ–¥–µ–ª—å
        best_model = min(self.results.keys(), 
                        key=lambda x: self.results[x]['test_mse'])
        best_mse = self.results[best_model]['test_mse']
        best_acc = self.results[best_model]['directional_accuracy']
        
        logger.info(f"\nüèÜ –õ–£–ß–®–ê–Ø –ú–û–î–ï–õ–¨: {best_model.upper()}")
        logger.info(f"   üìä MSE: {best_mse:.6f}")
        logger.info(f"   üéØ –¢–æ—á–Ω–æ—Å—Ç—å: {best_acc:.1f}%")
        
        # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
        logger.info(f"\nüí° –†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–ò:")
        if best_acc > 60:
            logger.info("   ‚úÖ –û—Ç–ª–∏—á–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã! –ú–æ–¥–µ–ª—å –≥–æ—Ç–æ–≤–∞ –¥–ª—è live-—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è")
        elif best_acc > 55:
            logger.info("   üëç –•–æ—Ä–æ—à–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã. –†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è")
        else:
            logger.info("   ‚ö†Ô∏è –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Ç—Ä–µ–±—É—é—Ç —É–ª—É—á—à–µ–Ω–∏—è. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –±–æ–ª—å—à–µ –¥–∞–Ω–Ω—ã—Ö –∏–ª–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤")
        
        logger.info("="*80)
    
    # ================== MAIN WORKFLOW ==================
    
    async def run_progressive_training(self, data_path: str = None,
                                     quick_mode: bool = False) -> Dict:
        """–ó–∞–ø—É—Å–∫ –ø–æ–ª–Ω–æ–≥–æ –ø–æ—ç—Ç–∞–ø–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è"""
        
        logger.info("üöÄ –ù–ê–ß–ò–ù–ê–ï–ú –ü–û–≠–¢–ê–ü–ù–û–ï –û–ë–£–ß–ï–ù–ò–ï –ú–û–î–ï–õ–ï–ô")
        logger.info("="*60)
        
        # –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
        df = self.load_data(data_path)
        X, y, feature_columns = self.prepare_data(df)
        
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
        if quick_mode:
            logger.info("‚ö° –ë–´–°–¢–†–´–ô –†–ï–ñ–ò–ú: —É–º–µ–Ω—å—à–µ–Ω–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ trials")
            xgb_trials, lstm_trials = 10, 5
        else:
            logger.info("üî• –ü–û–õ–ù–´–ô –†–ï–ñ–ò–ú: –º–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è")
            xgb_trials, lstm_trials = 50, 30
        
        all_results = {}
        
        try:
            # –≠–¢–ê–ü 1: XGBoost
            logger.info("\n" + "="*60)
            xgb_results = self.train_xgboost_phase(X, y, optimize=True, trials=xgb_trials)
            all_results['xgboost'] = xgb_results
            
            # –≠–¢–ê–ü 2: LSTM
            logger.info("\n" + "="*60)
            lstm_results = self.train_lstm_phase(X, y, optimize=True, trials=lstm_trials)
            all_results['lstm'] = lstm_results
            
            # –≠–¢–ê–ü 3: Ensemble
            logger.info("\n" + "="*60)
            ensemble_results = self.train_ensemble_phase(X, y)
            all_results['ensemble'] = ensemble_results
            
            # –°–æ–∑–¥–∞–Ω–∏–µ –≥—Ä–∞—Ñ–∏–∫–æ–≤
            logger.info("\n" + "="*60)
            logger.info("üìä –°–æ–∑–¥–∞–Ω–∏–µ –≥—Ä–∞—Ñ–∏–∫–æ–≤ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è...")
            self.create_comparison_plots()
            
            # –§–∏–Ω–∞–ª—å–Ω–æ–µ —Ä–µ–∑—é–º–µ
            self.print_final_summary()
            
            return all_results
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏: {e}")
            import traceback
            logger.error(traceback.format_exc())
            return all_results


# ================== –ì–õ–ê–í–ù–ê–Ø –§–£–ù–ö–¶–ò–Ø ==================

async def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏"""
    
    # –°–æ–∑–¥–∞–Ω–∏–µ —Ç—Ä–µ–Ω–µ—Ä–∞
    trainer = ProgressiveForexTrainer("EURUSD")
    
    # –ó–∞–ø—É—Å–∫ –ø–æ—ç—Ç–∞–ø–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è
    results = await trainer.run_progressive_training(
        data_path=None,  # None = —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–µ –¥–∞–Ω–Ω—ã–µ
        quick_mode=False  # True –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
    )
    
    logger.info("üéâ –ü–æ—ç—Ç–∞–ø–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ!")
    
    return results


if __name__ == "__main__":
    # –ó–∞–ø—É—Å–∫
    results = asyncio.run(main())