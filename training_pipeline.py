#!/usr/bin/env python3
"""
Training Pipeline for ForexBot AI Models
–°–∏—Å—Ç–µ–º–∞ –æ–±—É—á–µ–Ω–∏—è –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è AI –º–æ–¥–µ–ª–µ–π
"""

import numpy as np
import pandas as pd
import joblib
import pickle
import json
import os
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Tuple
from pathlib import Path
import logging
from sklearn.model_selection import TimeSeriesSplit
from sklearn.preprocessing import StandardScaler
import tensorflow as tf
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau
import xgboost as xgb
import lightgbm as lgb
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier

# –ò–º–ø–æ—Ä—Ç –Ω–∞—à–∏—Ö –º–æ–¥—É–ª–µ–π
from advanced_ai_models import (
    AdvancedFeatureEngineer,
    AdvancedLSTMModel,
    AdvancedTransformerModel,
    AdvancedEnsembleModel,
    create_advanced_models
)
from advanced_backtesting import AdvancedBacktester

logger = logging.getLogger(__name__)

class ModelTrainingPipeline:
    """–ü–∞–π–ø–ª–∞–π–Ω –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π"""
    
    def __init__(self, config: Dict):
        self.config = config
        self.models_dir = Path("data/models")
        self.models_dir.mkdir(parents=True, exist_ok=True)
        
        # –°–æ–∑–¥–∞–Ω–∏–µ –∏–Ω–∂–µ–Ω–µ—Ä–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        self.feature_engineer = AdvancedFeatureEngineer()
        
        # –°–æ–∑–¥–∞–Ω–∏–µ –∞–Ω—Å–∞–º–±–ª—è –º–æ–¥–µ–ª–µ–π
        self.ensemble_model = create_advanced_models()
        
        # Backtester –¥–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–∏
        self.backtester = AdvancedBacktester()
        
        # –ò—Å—Ç–æ—Ä–∏—è –æ–±—É—á–µ–Ω–∏—è
        self.training_history = {}
        
    def prepare_training_data(self, symbol: str, timeframe: str, 
                            start_date: str, end_date: str) -> Tuple[pd.DataFrame, np.ndarray]:
        """
        –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
        
        Args:
            symbol: –¢–æ—Ä–≥–æ–≤—ã–π –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç (–Ω–∞–ø—Ä–∏–º–µ—Ä, 'EURUSD')
            timeframe: –í—Ä–µ–º–µ–Ω–Ω–æ–π –∏–Ω—Ç–µ—Ä–≤–∞–ª ('M15', 'H1', 'H4')
            start_date: –î–∞—Ç–∞ –Ω–∞—á–∞–ª–∞ –≤ —Ñ–æ—Ä–º–∞—Ç–µ 'YYYY-MM-DD'
            end_date: –î–∞—Ç–∞ –æ–∫–æ–Ω—á–∞–Ω–∏—è –≤ —Ñ–æ—Ä–º–∞—Ç–µ 'YYYY-MM-DD'
            
        Returns:
            Tuple[pd.DataFrame, np.ndarray]: –ü–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –∏ —Ü–µ–ª–µ–≤–∞—è –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è
        """
        logger.info(f"üìä –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è {symbol} {timeframe}")
        
        # –ó–∞–≥—Ä—É–∑–∫–∞ –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö
        df = self._load_market_data(symbol, timeframe, start_date, end_date)
        
        if df.empty:
            raise ValueError(f"–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è {symbol} {timeframe}")
        
        # –°–æ–∑–¥–∞–Ω–∏–µ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–æ–≤
        df = self.feature_engineer.create_technical_indicators(df)
        
        # –°–æ–∑–¥–∞–Ω–∏–µ –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        df = self.feature_engineer.create_advanced_features(df)
        
        # –£–¥–∞–ª–µ–Ω–∏–µ NaN –∑–Ω–∞—á–µ–Ω–∏–π
        df = df.dropna()
        
        # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        features, feature_names = self.feature_engineer.prepare_features(df)
        
        # –°–æ–∑–¥–∞–Ω–∏–µ —Ü–µ–ª–µ–≤–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π
        target = self.ensemble_model._create_target_variable(df['close'])
        
        logger.info(f"‚úÖ –ü–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–æ {len(features)} –∑–∞–ø–∏—Å–µ–π —Å {len(feature_names)} –ø—Ä–∏–∑–Ω–∞–∫–∞–º–∏")
        
        return df, features, target, feature_names
    
    def _load_market_data(self, symbol: str, timeframe: str, 
                         start_date: str, end_date: str) -> pd.DataFrame:
        """–ó–∞–≥—Ä—É–∑–∫–∞ —Ä—ã–Ω–æ—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö"""
        # –ó–¥–µ—Å—å –¥–æ–ª–∂–Ω–∞ –±—ã—Ç—å —Ä–µ–∞–ª—å–Ω–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
        # –î–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏ —Å–æ–∑–¥–∞–µ–º —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–µ –¥–∞–Ω–Ω—ã–µ
        
        start_dt = datetime.strptime(start_date, '%Y-%m-%d')
        end_dt = datetime.strptime(end_date, '%Y-%m-%d')
        
        # –°–æ–∑–¥–∞–Ω–∏–µ –≤—Ä–µ–º–µ–Ω–Ω–æ–≥–æ —Ä—è–¥–∞
        if timeframe == 'M15':
            freq = '15T'
        elif timeframe == 'H1':
            freq = 'H'
        elif timeframe == 'H4':
            freq = '4H'
        else:
            freq = 'D'
        
        dates = pd.date_range(start_dt, end_dt, freq=freq)
        
        # –°–æ–∑–¥–∞–Ω–∏–µ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö
        np.random.seed(42)
        price = 100 + np.cumsum(np.random.randn(len(dates)) * 0.1
        volume = np.random.randint(1000, 10000, len(dates))
        
        df = pd.DataFrame({
            'open': price * (1 + np.random.randn(len(dates)) * 0.001),
            'high': price * (1 + abs(np.random.randn(len(dates)) * 0.002)),
            'low': price * (1 - abs(np.random.randn(len(dates)) * 0.002)),
            'close': price,
            'volume': volume
        }, index=dates)
        
        return df
    
    def train_models(self, symbol: str, timeframe: str, 
                    start_date: str, end_date: str,
                    validation_split: float = 0.2) -> Dict:
        """
        –û–±—É—á–µ–Ω–∏–µ –≤—Å–µ—Ö –º–æ–¥–µ–ª–µ–π
        
        Args:
            symbol: –¢–æ—Ä–≥–æ–≤—ã–π –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç
            timeframe: –í—Ä–µ–º–µ–Ω–Ω–æ–π –∏–Ω—Ç–µ—Ä–≤–∞–ª
            start_date: –î–∞—Ç–∞ –Ω–∞—á–∞–ª–∞ –æ–±—É—á–µ–Ω–∏—è
            end_date: –î–∞—Ç–∞ –æ–∫–æ–Ω—á–∞–Ω–∏—è –æ–±—É—á–µ–Ω–∏—è
            validation_split: –î–æ–ª—è –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–∏
            
        Returns:
            Dict: –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –æ–±—É—á–µ–Ω–∏—è
        """
        logger.info(f"üöÄ –ù–∞—á–∞–ª–æ –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π –¥–ª—è {symbol} {timeframe}")
        
        # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö
        df, features, target, feature_names = self.prepare_training_data(
            symbol, timeframe, start_date, end_date
        )
        
        # –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ –æ–±—É—á–∞—é—â—É—é –∏ –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω—É—é –≤—ã–±–æ—Ä–∫–∏
        split_idx = int(len(features) * (1 - validation_split))
        
        X_train = features[:split_idx]
        y_train = target[:split_idx]
        X_val = features[split_idx:]
        y_val = target[split_idx:]
        
        logger.info(f"üìà –û–±—É—á–∞—é—â–∞—è –≤—ã–±–æ—Ä–∫–∞: {len(X_train)} –∑–∞–ø–∏—Å–µ–π")
        logger.info(f"üìä –í–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω–∞—è –≤—ã–±–æ—Ä–∫–∞: {len(X_val)} –∑–∞–ø–∏—Å–µ–π")
        
        # –û–±—É—á–µ–Ω–∏–µ –∞–Ω—Å–∞–º–±–ª—è –º–æ–¥–µ–ª–µ–π
        training_results = self.ensemble_model.train_models(df.iloc[:split_idx])
        
        # –í–∞–ª–∏–¥–∞—Ü–∏—è –º–æ–¥–µ–ª–µ–π
        validation_results = self._validate_models(X_val, y_val)
        
        # Backtesting –Ω–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
        backtest_results = self._backtest_validation(df.iloc[split_idx:], X_val, y_val)
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π
        model_info = self._save_models(symbol, timeframe, training_results)
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –æ–±—É—á–µ–Ω–∏—è
        self._save_training_results(symbol, timeframe, {
            'training_results': training_results,
            'validation_results': validation_results,
            'backtest_results': backtest_results,
            'model_info': model_info,
            'feature_names': feature_names,
            'training_date': datetime.now().isoformat(),
            'data_info': {
                'symbol': symbol,
                'timeframe': timeframe,
                'start_date': start_date,
                'end_date': end_date,
                'train_samples': len(X_train),
                'val_samples': len(X_val),
                'features_count': len(feature_names)
            }
        })
        
        logger.info(f"‚úÖ –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ –¥–ª—è {symbol} {timeframe}")
        
        return {
            'training_results': training_results,
            'validation_results': validation_results,
            'backtest_results': backtest_results,
            'model_info': model_info
        }
    
    def _validate_models(self, X_val: np.ndarray, y_val: np.ndarray) -> Dict:
        """–í–∞–ª–∏–¥–∞—Ü–∏—è –º–æ–¥–µ–ª–µ–π"""
        validation_results = {}
        
        for model_name, model in self.ensemble_model.models.items():
            try:
                if hasattr(model, 'predict'):
                    predictions = model.predict(X_val)
                    
                    # –†–∞—Å—á–µ—Ç —Ç–æ—á–Ω–æ—Å—Ç–∏
                    if len(predictions.shape) > 1:
                        y_pred = np.argmax(predictions, axis=1)
                    else:
                        y_pred = predictions
                    
                    accuracy = np.mean(y_pred == y_val)
                    validation_results[model_name] = {
                        'accuracy': accuracy,
                        'predictions_shape': predictions.shape
                    }
                    
                    logger.info(f"üìä {model_name}: —Ç–æ—á–Ω–æ—Å—Ç—å = {accuracy:.4f}")
                    
            except Exception as e:
                logger.error(f"‚ùå –û—à–∏–±–∫–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏ {model_name}: {e}")
                validation_results[model_name] = {'error': str(e)}
        
        return validation_results
    
    def _backtest_validation(self, df_val: pd.DataFrame, 
                           X_val: np.ndarray, y_val: np.ndarray) -> Dict:
        """Backtesting –Ω–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö"""
        try:
            # –ü–æ–ª—É—á–µ–Ω–∏–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π –∞–Ω—Å–∞–º–±–ª—è
            ensemble_predictions = self.ensemble_model.predict_ensemble(df_val)
            predictions = ensemble_predictions['ensemble_prediction']
            
            # –ó–∞–ø—É—Å–∫ backtesting
            backtest_results = self.backtester.run_backtest(
                df_val, predictions, confidence_threshold=0.6
            )
            
            logger.info(f"üìà Backtesting —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã:")
            logger.info(f"   –û–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–¥–µ–ª–æ–∫: {backtest_results['total_trades']}")
            logger.info(f"   –í–∏–Ω—Ä–µ–π—Ç: {backtest_results['win_rate']:.2f}%")
            logger.info(f"   –û–±—â–∞—è –ø—Ä–∏–±—ã–ª—å: {backtest_results['total_profit']:.2f}")
            logger.info(f"   Sharpe Ratio: {backtest_results['sharpe_ratio']:.4f}")
            
            return backtest_results
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ backtesting: {e}")
            return {'error': str(e)}
    
    def _save_models(self, symbol: str, timeframe: str, training_results: Dict) -> Dict:
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –æ–±—É—á–µ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π"""
        model_info = {}
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        for model_name, result in training_results.items():
            try:
                model = self.ensemble_model.models.get(model_name)
                if model is not None:
                    # –°–æ–∑–¥–∞–Ω–∏–µ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ –¥–ª—è –º–æ–¥–µ–ª–∏
                    model_dir = self.models_dir / symbol / timeframe / model_name
                    model_dir.mkdir(parents=True, exist_ok=True)
                    
                    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
                    model_path = model_dir / f"{model_name}_{timestamp}.joblib"
                    
                    if hasattr(model, 'save_model'):
                        # –î–ª—è TensorFlow –º–æ–¥–µ–ª–µ–π
                        tf_model_path = model_dir / f"{model_name}_{timestamp}.h5"
                        model.save_model(str(tf_model_path))
                        joblib.dump(model, model_path)
                    else:
                        # –î–ª—è sklearn –º–æ–¥–µ–ª–µ–π
                        joblib.dump(model, model_path)
                    
                    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö
                    metadata = {
                        'model_name': model_name,
                        'symbol': symbol,
                        'timeframe': timeframe,
                        'training_date': datetime.now().isoformat(),
                        'model_path': str(model_path),
                        'training_metrics': result.get('metrics', {}),
                        'model_type': type(model).__name__
                    }
                    
                    metadata_path = model_dir / f"{model_name}_{timestamp}_metadata.json"
                    with open(metadata_path, 'w') as f:
                        json.dump(metadata, f, indent=2)
                    
                    model_info[model_name] = {
                        'path': str(model_path),
                        'metadata_path': str(metadata_path),
                        'training_metrics': result.get('metrics', {})
                    }
                    
                    logger.info(f"üíæ –ú–æ–¥–µ–ª—å {model_name} —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {model_path}")
                    
            except Exception as e:
                logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –º–æ–¥–µ–ª–∏ {model_name}: {e}")
                model_info[model_name] = {'error': str(e)}
        
        return model_info
    
    def _save_training_results(self, symbol: str, timeframe: str, results: Dict):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –æ–±—É—á–µ–Ω–∏—è"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # –°–æ–∑–¥–∞–Ω–∏–µ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ –¥–ª—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        results_dir = self.models_dir / symbol / timeframe / "training_results"
        results_dir.mkdir(parents=True, exist_ok=True)
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        results_path = results_dir / f"training_results_{timestamp}.json"
        with open(results_path, 'w') as f:
            json.dump(results, f, indent=2, default=str)
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∫—Ä–∞—Ç–∫–æ–≥–æ –æ—Ç—á–µ—Ç–∞
        summary = {
            'symbol': symbol,
            'timeframe': timeframe,
            'training_date': results['training_date'],
            'models_trained': list(results['model_info'].keys()),
            'validation_accuracy': {
                name: result.get('accuracy', 0) 
                for name, result in results['validation_results'].items()
                if 'accuracy' in result
            },
            'backtest_summary': {
                'total_trades': results['backtest_results'].get('total_trades', 0),
                'win_rate': results['backtest_results'].get('win_rate', 0),
                'total_profit': results['backtest_results'].get('total_profit', 0),
                'sharpe_ratio': results['backtest_results'].get('sharpe_ratio', 0)
            }
        }
        
        summary_path = results_dir / f"summary_{timestamp}.json"
        with open(summary_path, 'w') as f:
            json.dump(summary, f, indent=2)
        
        logger.info(f"üíæ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –æ–±—É—á–µ–Ω–∏—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: {results_path}")
        logger.info(f"üìã –ö—Ä–∞—Ç–∫–∏–π –æ—Ç—á–µ—Ç: {summary_path}")
    
    def load_models(self, symbol: str, timeframe: str, model_version: str = None) -> bool:
        """
        –ó–∞–≥—Ä—É–∑–∫–∞ –æ–±—É—á–µ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π
        
        Args:
            symbol: –¢–æ—Ä–≥–æ–≤—ã–π –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç
            timeframe: –í—Ä–µ–º–µ–Ω–Ω–æ–π –∏–Ω—Ç–µ—Ä–≤–∞–ª
            model_version: –í–µ—Ä—Å–∏—è –º–æ–¥–µ–ª–∏ (–µ—Å–ª–∏ None, –∑–∞–≥—Ä—É–∂–∞–µ—Ç—Å—è –ø–æ—Å–ª–µ–¥–Ω—è—è)
            
        Returns:
            bool: –£—Å–ø–µ—à–Ω–æ—Å—Ç—å –∑–∞–≥—Ä—É–∑–∫–∏
        """
        logger.info(f"üìÇ –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–µ–π –¥–ª—è {symbol} {timeframe}")
        
        try:
            model_dir = self.models_dir / symbol / timeframe
            
            if not model_dir.exists():
                logger.error(f"‚ùå –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –º–æ–¥–µ–ª–µ–π –Ω–µ –Ω–∞–π–¥–µ–Ω–∞: {model_dir}")
                return False
            
            # –ü–æ–∏—Å–∫ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π
            available_models = {}
            for model_name in self.ensemble_model.models.keys():
                model_subdir = model_dir / model_name
                if model_subdir.exists():
                    model_files = list(model_subdir.glob("*.joblib"))
                    if model_files:
                        # –í—ã–±–æ—Ä –≤–µ—Ä—Å–∏–∏ –º–æ–¥–µ–ª–∏
                        if model_version:
                            target_file = model_subdir / f"{model_name}_{model_version}.joblib"
                            if target_file.exists():
                                available_models[model_name] = target_file
                        else:
                            # –ü–æ—Å–ª–µ–¥–Ω—è—è –≤–µ—Ä—Å–∏—è
                            latest_file = max(model_files, key=lambda x: x.stat().st_mtime)
                            available_models[model_name] = latest_file
            
            if not available_models:
                logger.error(f"‚ùå –ù–µ –Ω–∞–π–¥–µ–Ω—ã –º–æ–¥–µ–ª–∏ –¥–ª—è {symbol} {timeframe}")
                return False
            
            # –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–µ–π
            for model_name, model_path in available_models.items():
                try:
                    model = joblib.load(model_path)
                    self.ensemble_model.models[model_name] = model
                    logger.info(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–∞ –º–æ–¥–µ–ª—å {model_name}: {model_path}")
                    
                    # –ó–∞–≥—Ä—É–∑–∫–∞ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö
                    metadata_path = model_path.with_suffix('_metadata.json')
                    if metadata_path.exists():
                        with open(metadata_path, 'r') as f:
                            metadata = json.load(f)
                        logger.info(f"üìã –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ {model_name}: {metadata.get('training_date', 'N/A')}")
                    
                except Exception as e:
                    logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏ {model_name}: {e}")
            
            # –ó–∞–≥—Ä—É–∑–∫–∞ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –ø—Ä–∏–∑–Ω–∞–∫–∞—Ö
            feature_info_path = model_dir / "feature_info.json"
            if feature_info_path.exists():
                with open(feature_info_path, 'r') as f:
                    feature_info = json.load(f)
                self.ensemble_model.feature_engineer.feature_names = feature_info.get('feature_names', [])
                logger.info(f"üìä –ó–∞–≥—Ä—É–∂–µ–Ω–∞ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ {len(self.ensemble_model.feature_engineer.feature_names)} –ø—Ä–∏–∑–Ω–∞–∫–∞—Ö")
            
            logger.info(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(available_models)} –º–æ–¥–µ–ª–µ–π")
            return True
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–µ–π: {e}")
            return False
    
    def get_model_info(self, symbol: str, timeframe: str) -> Dict:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –º–æ–¥–µ–ª—è—Ö"""
        model_dir = self.models_dir / symbol / timeframe
        
        if not model_dir.exists():
            return {}
        
        model_info = {}
        
        for model_subdir in model_dir.iterdir():
            if model_subdir.is_dir() and model_subdir.name != "training_results":
                model_name = model_subdir.name
                model_files = list(model_subdir.glob("*.joblib"))
                
                if model_files:
                    latest_file = max(model_files, key=lambda x: x.stat().st_mtime)
                    metadata_path = latest_file.with_suffix('_metadata.json')
                    
                    info = {
                        'model_path': str(latest_file),
                        'last_modified': datetime.fromtimestamp(latest_file.stat().st_mtime).isoformat(),
                        'file_size': latest_file.stat().st_size
                    }
                    
                    if metadata_path.exists():
                        with open(metadata_path, 'r') as f:
                            metadata = json.load(f)
                        info.update(metadata)
                    
                    model_info[model_name] = info
        
        return model_info
    
    def retrain_models(self, symbol: str, timeframe: str, 
                      days_back: int = 365) -> Dict:
        """
        –ü–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π –Ω–∞ –Ω–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö
        
        Args:
            symbol: –¢–æ—Ä–≥–æ–≤—ã–π –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç
            timeframe: –í—Ä–µ–º–µ–Ω–Ω–æ–π –∏–Ω—Ç–µ—Ä–≤–∞–ª
            days_back: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–Ω–µ–π –Ω–∞–∑–∞–¥ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
            
        Returns:
            Dict: –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è
        """
        logger.info(f"üîÑ –ü–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π –¥–ª—è {symbol} {timeframe}")
        
        # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–µ—Ä–∏–æ–¥–∞ –æ–±—É—á–µ–Ω–∏—è
        end_date = datetime.now()
        start_date = end_date - timedelta(days=days_back)
        
        # –û–±—É—á–µ–Ω–∏–µ –Ω–∞ –Ω–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö
        results = self.train_models(
            symbol, timeframe,
            start_date.strftime('%Y-%m-%d'),
            end_date.strftime('%Y-%m-%d')
        )
        
        logger.info(f"‚úÖ –ü–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ –¥–ª—è {symbol} {timeframe}")
        
        return results

# –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
if __name__ == "__main__":
    # –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
    config = {
        "ai": {
            "models": ["lstm", "xgboost", "lightgbm"],
            "timeframes": ["M15", "H1", "H4"],
            "lookback_periods": [50, 100, 200],
            "retrain_interval": 24,
            "min_accuracy_threshold": 0.65
        }
    }
    
    # –°–æ–∑–¥–∞–Ω–∏–µ –ø–∞–π–ø–ª–∞–π–Ω–∞ –æ–±—É—á–µ–Ω–∏—è
    pipeline = ModelTrainingPipeline(config)
    
    # –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π
    print("üöÄ –ù–∞—á–∞–ª–æ –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π...")
    
    results = pipeline.train_models(
        symbol="EURUSD",
        timeframe="H1",
        start_date="2023-01-01",
        end_date="2023-12-31"
    )
    
    print("‚úÖ –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ!")
    print(f"üìä –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤–∞–ª–∏–¥–∞—Ü–∏–∏:")
    for model_name, result in results['validation_results'].items():
        if 'accuracy' in result:
            print(f"   {model_name}: {result['accuracy']:.4f}")
    
    print(f"üìà Backtesting —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã:")
    backtest = results['backtest_results']
    print(f"   –û–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–¥–µ–ª–æ–∫: {backtest.get('total_trades', 0)}")
    print(f"   –í–∏–Ω—Ä–µ–π—Ç: {backtest.get('win_rate', 0):.2f}%")
    print(f"   –û–±—â–∞—è –ø—Ä–∏–±—ã–ª—å: {backtest.get('total_profit', 0):.2f}")
    print(f"   Sharpe Ratio: {backtest.get('sharpe_ratio', 0):.4f}")
    
    # –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–µ–π
    print("\nüìÇ –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–µ–π...")
    success = pipeline.load_models("EURUSD", "H1")
    if success:
        print("‚úÖ –ú–æ–¥–µ–ª–∏ –∑–∞–≥—Ä—É–∂–µ–Ω—ã —É—Å–ø–µ—à–Ω–æ!")
    else:
        print("‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–µ–π")
    
    # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –º–æ–¥–µ–ª—è—Ö
    model_info = pipeline.get_model_info("EURUSD", "H1")
    print(f"\nüìã –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –º–æ–¥–µ–ª—è—Ö:")
    for model_name, info in model_info.items():
        print(f"   {model_name}: {info.get('training_date', 'N/A')}")