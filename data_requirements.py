#!/usr/bin/env python3
"""
Data Requirements for Professional AI Training
–¢—Ä–µ–±–æ–≤–∞–Ω–∏—è –∫ –¥–∞–Ω–Ω—ã–º –¥–ª—è –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è AI
"""

import pandas as pd
import numpy as np
from datetime import datetime, timedelta
from typing import Dict, List, Tuple
import yfinance as yf
import requests
import json

class DataRequirements:
    """–¢—Ä–µ–±–æ–≤–∞–Ω–∏—è –∫ –¥–∞–Ω–Ω—ã–º –¥–ª—è –æ–±—É—á–µ–Ω–∏—è AI –º–æ–¥–µ–ª–µ–π"""
    
    def __init__(self):
        # –†–µ–∫–æ–º–µ–Ω–¥—É–µ–º—ã–µ –≤–∞–ª—é—Ç–Ω—ã–µ –ø–∞—Ä—ã
        self.major_pairs = [
            "EURUSD", "GBPUSD", "USDJPY", "USDCHF", 
            "AUDUSD", "USDCAD", "NZDUSD"
        ]
        
        self.cross_pairs = [
            "EURGBP", "EURJPY", "GBPJPY", "AUDJPY",
            "EURAUD", "GBPAUD", "AUDCAD", "CADJPY"
        ]
        
        self.exotic_pairs = [
            "USDTRY", "USDZAR", "EURTRY", "GBPZAR",
            "USDSGD", "USDHKD", "USDMXN", "USDBRL"
        ]
        
        # –¢–∞–π–º—Ñ—Ä–µ–π–º—ã –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
        self.timeframes = {
            "M1": "1m",      # 1 –º–∏–Ω—É—Ç–∞
            "M5": "5m",      # 5 –º–∏–Ω—É—Ç
            "M15": "15m",    # 15 –º–∏–Ω—É—Ç
            "M30": "30m",    # 30 –º–∏–Ω—É—Ç
            "H1": "1h",      # 1 —á–∞—Å
            "H4": "4h",      # 4 —á–∞—Å–∞
            "D1": "1d",      # 1 –¥–µ–Ω—å
            "W1": "1wk",     # 1 –Ω–µ–¥–µ–ª—è
            "MN1": "1mo"     # 1 –º–µ—Å—è—Ü
        }
        
        # –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–µ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è –∫ –¥–∞–Ω–Ω—ã–º
        self.min_data_requirements = {
            "training_samples": 10000,    # –ú–∏–Ω–∏–º—É–º 10,000 –∑–∞–ø–∏—Å–µ–π –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
            "validation_samples": 2000,   # –ú–∏–Ω–∏–º—É–º 2,000 –∑–∞–ø–∏—Å–µ–π –¥–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–∏
            "test_samples": 2000,         # –ú–∏–Ω–∏–º—É–º 2,000 –∑–∞–ø–∏—Å–µ–π –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
            "min_years": 3,               # –ú–∏–Ω–∏–º—É–º 3 –≥–æ–¥–∞ –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö
            "min_quality_score": 0.95     # –ú–∏–Ω–∏–º—É–º 95% –∫–∞—á–µ—Å—Ç–≤–æ –¥–∞–Ω–Ω—ã—Ö
        }
    
    def get_recommended_pairs(self, category: str = "all") -> List[str]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ–º—ã—Ö –≤–∞–ª—é—Ç–Ω—ã—Ö –ø–∞—Ä"""
        if category == "major":
            return self.major_pairs
        elif category == "cross":
            return self.cross_pairs
        elif category == "exotic":
            return self.exotic_pairs
        else:
            return self.major_pairs + self.cross_pairs + self.exotic_pairs
    
    def get_timeframe_recommendations(self, strategy_type: str = "general") -> Dict:
        """–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ —Ç–∞–π–º—Ñ—Ä–µ–π–º–∞–º –¥–ª—è —Ä–∞–∑–Ω—ã—Ö —Å—Ç—Ä–∞—Ç–µ–≥–∏–π"""
        
        recommendations = {
            "scalping": {
                "primary": ["M1", "M5", "M15"],
                "secondary": ["M30", "H1"],
                "description": "–°–∫–∞–ª—å–ø–∏–Ω–≥ —Ç—Ä–µ–±—É–µ—Ç –≤—ã—Å–æ–∫–æ—á–∞—Å—Ç–æ—Ç–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö",
                "min_samples": 50000,
                "min_years": 1
            },
            "day_trading": {
                "primary": ["M15", "M30", "H1"],
                "secondary": ["H4", "D1"],
                "description": "–î–Ω–µ–≤–Ω–∞—è —Ç–æ—Ä–≥–æ–≤–ª—è –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —Å—Ä–µ–¥–Ω–∏–µ —Ç–∞–π–º—Ñ—Ä–µ–π–º—ã",
                "min_samples": 20000,
                "min_years": 2
            },
            "swing_trading": {
                "primary": ["H1", "H4", "D1"],
                "secondary": ["W1", "MN1"],
                "description": "–°–≤–∏–Ω–≥-—Ç—Ä–µ–π–¥–∏–Ω–≥ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —Å—Ç–∞—Ä—à–∏–µ —Ç–∞–π–º—Ñ—Ä–µ–π–º—ã",
                "min_samples": 15000,
                "min_years": 3
            },
            "position_trading": {
                "primary": ["H4", "D1", "W1"],
                "secondary": ["MN1"],
                "description": "–ü–æ–∑–∏—Ü–∏–æ–Ω–Ω–∞—è —Ç–æ—Ä–≥–æ–≤–ª—è —Ç—Ä–µ–±—É–µ—Ç –¥–æ–ª–≥–æ—Å—Ä–æ—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö",
                "min_samples": 10000,
                "min_years": 5
            },
            "general": {
                "primary": ["M15", "H1", "H4"],
                "secondary": ["D1"],
                "description": "–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–∞—è —Å—Ç—Ä–∞—Ç–µ–≥–∏—è",
                "min_samples": 15000,
                "min_years": 3
            }
        }
        
        return recommendations.get(strategy_type, recommendations["general"])
    
    def calculate_data_requirements(self, 
                                 pairs: List[str], 
                                 timeframes: List[str],
                                 strategy_type: str = "general") -> Dict:
        """–†–∞—Å—á–µ—Ç —Ç—Ä–µ–±–æ–≤–∞–Ω–∏–π –∫ –¥–∞–Ω–Ω—ã–º"""
        
        timeframe_rec = self.get_timeframe_recommendations(strategy_type)
        
        # –†–∞—Å—á–µ—Ç –æ–±—â–µ–≥–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –¥–∞–Ω–Ω—ã—Ö
        total_pairs = len(pairs)
        total_timeframes = len(timeframes)
        
        # –û—Ü–µ–Ω–∫–∞ —Ä–∞–∑–º–µ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö
        samples_per_pair_timeframe = timeframe_rec["min_samples"]
        total_samples = total_pairs * total_timeframes * samples_per_pair_timeframe
        
        # –û—Ü–µ–Ω–∫–∞ –≤—Ä–µ–º–µ–Ω–∏ –∑–∞–≥—Ä—É–∑–∫–∏
        estimated_download_time = total_samples / 1000  # —Å–µ–∫—É–Ω–¥—ã (–ø—Ä–∏–º–µ—Ä–Ω–æ)
        
        # –û—Ü–µ–Ω–∫–∞ —Ä–∞–∑–º–µ—Ä–∞ —Ñ–∞–π–ª–æ–≤
        bytes_per_record = 100  # –ø—Ä–∏–º–µ—Ä–Ω—ã–π —Ä–∞–∑–º–µ—Ä –æ–¥–Ω–æ–π –∑–∞–ø–∏—Å–∏
        total_size_mb = (total_samples * bytes_per_record) / (1024 * 1024)
        
        return {
            "total_pairs": total_pairs,
            "total_timeframes": total_timeframes,
            "samples_per_pair_timeframe": samples_per_pair_timeframe,
            "total_samples": total_samples,
            "estimated_download_time_minutes": estimated_download_time / 60,
            "estimated_size_mb": total_size_mb,
            "min_years_required": timeframe_rec["min_years"],
            "strategy_type": strategy_type,
            "recommended_timeframes": timeframe_rec
        }
    
    def validate_data_quality(self, df: pd.DataFrame) -> Dict:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –¥–∞–Ω–Ω—ã—Ö"""
        
        quality_report = {
            "total_records": len(df),
            "date_range": {
                "start": df.index.min() if not df.empty else None,
                "end": df.index.max() if not df.empty else None
            },
            "missing_data": {},
            "duplicates": 0,
            "outliers": {},
            "quality_score": 0.0,
            "issues": []
        }
        
        if df.empty:
            quality_report["issues"].append("DataFrame –ø—É—Å—Ç–æ–π")
            return quality_report
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
        for col in ['open', 'high', 'low', 'close', 'volume']:
            if col in df.columns:
                missing_count = df[col].isnull().sum()
                missing_percent = (missing_count / len(df)) * 100
                quality_report["missing_data"][col] = {
                    "count": missing_count,
                    "percent": missing_percent
                }
                
                if missing_percent > 5:
                    quality_report["issues"].append(f"–ú–Ω–æ–≥–æ –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –≤ {col}: {missing_percent:.2f}%")
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤
        duplicates = df.duplicated().sum()
        quality_report["duplicates"] = duplicates
        
        if duplicates > 0:
            quality_report["issues"].append(f"–û–±–Ω–∞—Ä—É–∂–µ–Ω—ã –¥—É–±–ª–∏–∫–∞—Ç—ã: {duplicates}")
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –≤—ã–±—Ä–æ—Å–æ–≤
        for col in ['open', 'high', 'low', 'close']:
            if col in df.columns:
                Q1 = df[col].quantile(0.25)
                Q3 = df[col].quantile(0.75)
                IQR = Q3 - Q1
                outliers = ((df[col] < (Q1 - 1.5 * IQR)) | (df[col] > (Q3 + 1.5 * IQR))).sum()
                outlier_percent = (outliers / len(df)) * 100
                
                quality_report["outliers"][col] = {
                    "count": outliers,
                    "percent": outlier_percent
                }
                
                if outlier_percent > 10:
                    quality_report["issues"].append(f"–ú–Ω–æ–≥–æ –≤—ã–±—Ä–æ—Å–æ–≤ –≤ {col}: {outlier_percent:.2f}%")
        
        # –†–∞—Å—á–µ—Ç –æ–±—â–µ–≥–æ –∫–∞—á–µ—Å—Ç–≤–∞
        total_issues = len(quality_report["issues"])
        max_issues = 10  # –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–æ–±–ª–µ–º
        quality_report["quality_score"] = max(0, 1 - (total_issues / max_issues))
        
        return quality_report
    
    def get_data_sources(self) -> Dict:
        """–†–µ–∫–æ–º–µ–Ω–¥—É–µ–º—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏ –¥–∞–Ω–Ω—ã—Ö"""
        
        return {
            "free_sources": {
                "yfinance": {
                    "description": "–ë–µ—Å–ø–ª–∞—Ç–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ Yahoo Finance",
                    "coverage": "–û—Å–Ω–æ–≤–Ω—ã–µ –≤–∞–ª—é—Ç–Ω—ã–µ –ø–∞—Ä—ã",
                    "quality": "–°—Ä–µ–¥–Ω—è—è",
                    "update_frequency": "Real-time",
                    "limitations": "–û–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω–∞—è –∏—Å—Ç–æ—Ä–∏—è, –≤–æ–∑–º–æ–∂–Ω—ã –ø—Ä–æ–ø—É—Å–∫–∏"
                },
                "alpha_vantage": {
                    "description": "–ë–µ—Å–ø–ª–∞—Ç–Ω—ã–π API —Å –ª–∏–º–∏—Ç–∞–º–∏",
                    "coverage": "–®–∏—Ä–æ–∫–∏–π —Å–ø–µ–∫—Ç—Ä –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤",
                    "quality": "–•–æ—Ä–æ—à–∞—è",
                    "update_frequency": "Real-time",
                    "limitations": "–õ–∏–º–∏—Ç –∑–∞–ø—Ä–æ—Å–æ–≤, –ø–ª–∞—Ç–Ω–∞—è –ø–æ–¥–ø–∏—Å–∫–∞ –¥–ª—è –±–æ–ª—å—à–∏—Ö –æ–±—ä–µ–º–æ–≤"
                },
                "quandl": {
                    "description": "–ë–µ—Å–ø–ª–∞—Ç–Ω—ã–µ —ç–∫–æ–Ω–æ–º–∏—á–µ—Å–∫–∏–µ –¥–∞–Ω–Ω—ã–µ",
                    "coverage": "–ú–∞–∫—Ä–æ—ç–∫–æ–Ω–æ–º–∏—á–µ—Å–∫–∏–µ –¥–∞–Ω–Ω—ã–µ",
                    "quality": "–í—ã—Å–æ–∫–∞—è",
                    "update_frequency": "Daily",
                    "limitations": "–û–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω—ã–µ –≤–∞–ª—é—Ç–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ"
                }
            },
            "paid_sources": {
                "fxcm": {
                    "description": "–ü—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ FXCM",
                    "coverage": "–ü–æ–ª–Ω—ã–π —Å–ø–µ–∫—Ç—Ä –≤–∞–ª—é—Ç–Ω—ã—Ö –ø–∞—Ä",
                    "quality": "–û—á–µ–Ω—å –≤—ã—Å–æ–∫–∞—è",
                    "update_frequency": "Tick data",
                    "cost": "–û—Ç $50/–º–µ—Å—è—Ü"
                },
                "oanda": {
                    "description": "–î–∞–Ω–Ω—ã–µ OANDA",
                    "coverage": "–®–∏—Ä–æ–∫–∏–π —Å–ø–µ–∫—Ç—Ä –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤",
                    "quality": "–í—ã—Å–æ–∫–∞—è",
                    "update_frequency": "Real-time",
                    "cost": "–û—Ç $100/–º–µ—Å—è—Ü"
                },
                "dukascopy": {
                    "description": "–ò—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏–µ –¥–∞–Ω–Ω—ã–µ Dukascopy",
                    "coverage": "–û—Å–Ω–æ–≤–Ω—ã–µ –≤–∞–ª—é—Ç–Ω—ã–µ –ø–∞—Ä—ã",
                    "quality": "–í—ã—Å–æ–∫–∞—è",
                    "update_frequency": "Historical",
                    "cost": "–ë–µ—Å–ø–ª–∞—Ç–Ω–æ –¥–ª—è –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö"
                },
                "bloomberg": {
                    "description": "–ü—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω–∞—è –ø–ª–∞—Ç—Ñ–æ—Ä–º–∞ Bloomberg",
                    "coverage": "–í—Å–µ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã",
                    "quality": "–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è",
                    "update_frequency": "Real-time",
                    "cost": "–û—Ç $2000/–º–µ—Å—è—Ü"
                }
            }
        }
    
    def generate_data_plan(self, 
                          strategy_type: str = "general",
                          budget: str = "free") -> Dict:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –ø–ª–∞–Ω–∞ —Å–±–æ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö"""
        
        # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ –ø–∞—Ä–∞–º
        if strategy_type == "scalping":
            recommended_pairs = self.major_pairs[:3]  # EURUSD, GBPUSD, USDJPY
        elif strategy_type == "day_trading":
            recommended_pairs = self.major_pairs + self.cross_pairs[:2]
        else:
            recommended_pairs = self.major_pairs + self.cross_pairs[:4]
        
        # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ —Ç–∞–π–º—Ñ—Ä–µ–π–º–∞–º
        timeframe_rec = self.get_timeframe_recommendations(strategy_type)
        recommended_timeframes = timeframe_rec["primary"]
        
        # –†–∞—Å—á–µ—Ç —Ç—Ä–µ–±–æ–≤–∞–Ω–∏–π
        requirements = self.calculate_data_requirements(
            recommended_pairs, recommended_timeframes, strategy_type
        )
        
        # –í—ã–±–æ—Ä –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ –¥–∞–Ω–Ω—ã—Ö
        data_sources = self.get_data_sources()
        if budget == "free":
            sources = list(data_sources["free_sources"].keys())
        else:
            sources = list(data_sources["paid_sources"].keys())
        
        return {
            "strategy_type": strategy_type,
            "recommended_pairs": recommended_pairs,
            "recommended_timeframes": recommended_timeframes,
            "data_requirements": requirements,
            "data_sources": sources,
            "budget": budget,
            "estimated_cost": "Free" if budget == "free" else "$50-200/month",
            "implementation_steps": [
                "1. –í—ã–±–µ—Ä–∏—Ç–µ –∏—Å—Ç–æ—á–Ω–∏–∫ –¥–∞–Ω–Ω—ã—Ö –∏–∑ —Å–ø–∏—Å–∫–∞",
                "2. –ó–∞—Ä–µ–≥–∏—Å—Ç—Ä–∏—Ä—É–π—Ç–µ—Å—å –∏ –ø–æ–ª—É—á–∏—Ç–µ API –∫–ª—é—á–∏",
                "3. –°–∫–∞—á–∞–π—Ç–µ –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏–µ –¥–∞–Ω–Ω—ã–µ",
                "4. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –∫–∞—á–µ—Å—Ç–≤–æ –¥–∞–Ω–Ω—ã—Ö",
                "5. –ü–æ–¥–≥–æ—Ç–æ–≤—å—Ç–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è"
            ]
        }

# –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
if __name__ == "__main__":
    dr = DataRequirements()
    
    print("üìä –ê–Ω–∞–ª–∏–∑ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏–π –∫ –¥–∞–Ω–Ω—ã–º –¥–ª—è –æ–±—É—á–µ–Ω–∏—è AI –º–æ–¥–µ–ª–µ–π")
    print("=" * 60)
    
    # –ê–Ω–∞–ª–∏–∑ –¥–ª—è —Ä–∞–∑–Ω—ã—Ö —Å—Ç—Ä–∞—Ç–µ–≥–∏–π
    strategies = ["scalping", "day_trading", "swing_trading", "position_trading"]
    
    for strategy in strategies:
        print(f"\nüéØ –°—Ç—Ä–∞—Ç–µ–≥–∏—è: {strategy.upper()}")
        plan = dr.generate_data_plan(strategy, "free")
        
        print(f"–†–µ–∫–æ–º–µ–Ω–¥—É–µ–º—ã–µ –ø–∞—Ä—ã: {', '.join(plan['recommended_pairs'])}")
        print(f"–†–µ–∫–æ–º–µ–Ω–¥—É–µ–º—ã–µ —Ç–∞–π–º—Ñ—Ä–µ–π–º—ã: {', '.join(plan['recommended_timeframes'])}")
        print(f"–û–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∑–∞–ø–∏—Å–µ–π: {plan['data_requirements']['total_samples']:,}")
        print(f"–ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π –ø–µ—Ä–∏–æ–¥: {plan['data_requirements']['min_years_required']} –ª–µ—Ç")
        print(f"–ü—Ä–∏–º–µ—Ä–Ω—ã–π —Ä–∞–∑–º–µ—Ä –¥–∞–Ω–Ω—ã—Ö: {plan['data_requirements']['estimated_size_mb']:.1f} MB")
    
    print("\n" + "=" * 60)
    print("üìã –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º –¥–∞–Ω–Ω—ã—Ö:")
    
    sources = dr.get_data_sources()
    for category, source_list in sources.items():
        print(f"\n{category.upper()}:")
        for name, info in source_list.items():
            print(f"  {name}: {info['description']} - {info.get('cost', 'Free')}")
    
    print("\nüéØ –î–ª—è –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è:")
    print("1. –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ø–ª–∞—Ç–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏ –¥–∞–Ω–Ω—ã—Ö (FXCM, OANDA)")
    print("2. –°–æ–±—Ä–∞—Ç—å –º–∏–Ω–∏–º—É–º 3-5 –ª–µ—Ç –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö")
    print("3. –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –Ω–µ—Å–∫–æ–ª—å–∫–æ —Ç–∞–π–º—Ñ—Ä–µ–π–º–æ–≤ (H1, H4, D1)")
    print("4. –û–±—É—á–∞—Ç—å—Å—è –Ω–∞ –æ—Å–Ω–æ–≤–Ω—ã—Ö –≤–∞–ª—é—Ç–Ω—ã—Ö –ø–∞—Ä–∞—Ö")
    print("5. –†–µ–≥—É–ª—è—Ä–Ω–æ –æ–±–Ω–æ–≤–ª—è—Ç—å –¥–∞–Ω–Ω—ã–µ –∏ –ø–µ—Ä–µ–æ–±—É—á–∞—Ç—å –º–æ–¥–µ–ª–∏")